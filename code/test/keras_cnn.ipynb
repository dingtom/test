{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "Vf6uB2pge6Kc",
    "outputId": "d2a3702c-2701-45d7-c0f0-0e83978cb99c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyts==0.7.1 in /home/zut_csi/anaconda3/envs/tomding/lib/python3.8/site-packages (0.7.1)\n",
      "Requirement already satisfied: numpy>=1.8.2scipy>=0.13.3scikit-learn>=0.17.0future>=0.13.1 in /home/zut_csi/anaconda3/envs/tomding/lib/python3.8/site-packages (from pyts==0.7.1) (1.19.4)\n",
      "model name\t: Intel(R) Core(TM) i9-10900F CPU @ 2.80GHz\n",
      "Tue Dec 22 10:27:55 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3070    Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   36C    P8    10W / 270W |    281MiB /  7979MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1584      G   /usr/lib/xorg/Xorg                 24MiB |\n",
      "|    0   N/A  N/A      1679      G   /usr/bin/gnome-shell               53MiB |\n",
      "|    0   N/A  N/A     13465      G   /usr/lib/xorg/Xorg                128MiB |\n",
      "|    0   N/A  N/A     13602      G   /usr/bin/gnome-shell               67MiB |\n",
      "|    0   N/A  N/A     13637      G   ...mviewer/tv_bin/TeamViewer        3MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.utils import plot_model\n",
    "from struct import unpack, pack\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import scipy.io as scio\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import interpolate\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "from tqdm import tqdm\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy.fftpack import fft\n",
    "from random import shuffle\n",
    "from collections import Counter  \n",
    "from sklearn.ensemble import IsolationForest \n",
    "from sklearn.model_selection import train_test_split\n",
    "!pip install pyts==0.7.1\n",
    "from pyts.image import *\n",
    "\n",
    "!cat /proc/cpuinfo | grep 'model name' |uniq\n",
    "!nvidia-smi\n",
    "    \n",
    "# 设置GPU内存按需分配\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取CSI中想要的部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kK6ZE6xCe6K6"
   },
   "source": [
    "# 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_x6ikrne6K8"
   },
   "outputs": [],
   "source": [
    "# 添加CTC损失函数\n",
    "def ctc_lambda(args):\n",
    "    labels, y_pred, input_length, label_length = args\n",
    "    y_pred = y_pred[:, :, :]\n",
    "    return tf.keras.backend.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "# 定义解码器\n",
    "\n",
    "\n",
    "def decode_ctc(preds, py_list):\n",
    "    window_num = np.zeros((1), dtype=np.int32)\n",
    "    window_num[0] = preds.shape[1]\n",
    "    decode = keras.backend.ctc_decode(\n",
    "        preds, window_num, greedy=True, beam_width=100, top_paths=1)\n",
    "    result_index = keras.backend.get_value(decode[0][0])[0]\n",
    "    result_py = []\n",
    "    for i in result_index:\n",
    "        try:\n",
    "            result_py.append(py_list[i])\n",
    "        except IndexError:\n",
    "            print(i, 'not in py_list')\n",
    "    return result_index, result_py\n",
    "\n",
    "\n",
    "def create_model(input_size, output_size):\n",
    "    inputs = Input(name='the_inputs', shape=input_size)\n",
    "    # 1\n",
    "    h1_1 = Conv2D(64, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_1-1')(inputs)\n",
    "    h1_2 = BatchNormalization(name='BatchNormal_1-1')(h1_1)\n",
    "    h1_3 = Conv2D(64, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_1-2')(h1_2)\n",
    "    h1_4 = BatchNormalization(name='BatchNormal_1-2')(h1_3)\n",
    "    h1_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_1')(h1_4)\n",
    "    # 2\n",
    "    h2_1 = Conv2D(128, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_2-1')(h1_5)\n",
    "    h2_2 = BatchNormalization(name='BatchNormal_2-1')(h2_1)\n",
    "    h2_3 = Conv2D(128, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_2-2')(h2_2)\n",
    "    h2_4 = BatchNormalization(name='BatchNormal_2-2')(h2_3)\n",
    "    h2_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_2')(h2_4)\n",
    "    # 3\n",
    "    h3_1 = Conv2D(256, (3, 3), use_bias=True, activation='relu', padding='same',\n",
    "                  kernel_initializer='he_normal', name='Conv2D_3-1')(h2_5)\n",
    "    h3_2 = BatchNormalization(name='BatchNormal_3-1')(h3_1)\n",
    "    h3_3 = Conv2D(256, (3, 3), use_bias=True, activation='relu', padding='same',\n",
    "                  kernel_initializer='he_normal', name='Conv2D_3-2')(h3_2)\n",
    "    h3_4 = BatchNormalization(name='BatchNormal_3-2')(h3_3)\n",
    "    h3_5 = MaxPooling2D(pool_size=(2, 2), strides=None,\n",
    "                        padding=\"valid\", name='MaxPooling2D_3')(h3_4)\n",
    "    # 4\n",
    "    h4_1 = Conv2D(512, (3, 3), use_bias=True, activation='relu', padding='same',\n",
    "                  kernel_initializer='he_normal', name='Conv2D_4-1')(h3_5)\n",
    "    h4_2 = BatchNormalization(name='BatchNormal_4-1')(h4_1)\n",
    "    h4_3 = Conv2D(512, (3, 3), use_bias=True, activation='relu', padding='same',\n",
    "                  kernel_initializer='he_normal', name='Conv2D_4-2')(h4_2)\n",
    "    h4_4 = BatchNormalization(name='BatchNormal_4-2')(h4_3)\n",
    "    # 由于声学模型网络结构原因（3个maxpooling层），我们的音频数据的每个维度需要能够被8整除。这里输入序列经过卷积网络后，长度缩短了8倍，因此我们训练实际输入的数据为wav_len//8。\n",
    "    h5_1 = Reshape((-1, int(input_size[1]//8*512)), name='Reshape_1')(h4_4)\n",
    "    lstm_1 = LSTM(128, return_sequences=True, kernel_initializer='he_normal', name='lstm1')(h5_1)\n",
    "    lstm_2 = LSTM(256, return_sequences=True, kernel_initializer='he_normal', name='lstm2')(lstm_1)\n",
    "    h5_2 = Dense(512, activation='relu', use_bias=True, kernel_initializer='he_normal', name='Dense_1')(lstm_2)\n",
    "    h5_3 = Dense(output_size, activation=\"relu\", use_bias=True, kernel_initializer='he_normal', name='Dense_2')(h5_2)  # (layer_h15)\n",
    "\n",
    "    outputs = Activation('softmax', name='Activation_1')(h5_3)\n",
    "    base_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    labels = Input(name='the_labels', shape=[None], dtype='float32')\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "    # keras.Lambda(function, output_shape=None, mask=None, arguments=None)\n",
    "    # 将任意表达式封装为 Layer 对象\n",
    "    loss_out = Lambda(ctc_lambda, output_shape=(1,), name='ctc')([labels, outputs, input_length, label_length])\n",
    "    ctc_model = keras.Model(inputs=[labels, inputs, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.0008, beta_1=0.9, beta_2=0.999, decay=0.01, epsilon=10e-8)\n",
    "    # ctc_model=multi_gpu_model(ctc_model,gpus=2)\n",
    "    ctc_model.compile(loss={'ctc': lambda y_true, output: output}, optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return base_model, ctc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1cxbqBf0e6Li"
   },
   "source": [
    "# 开始训练、测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7SKCjWxKQeSt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_inputs (InputLayer)         [(None, None, 64, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_1-1 (Conv2D)             (None, None, 64, 64) 640         the_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_1-1 (BatchNormaliza (None, None, 64, 64) 256         Conv2D_1-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_1-2 (Conv2D)             (None, None, 64, 64) 36928       BatchNormal_1-1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_1-2 (BatchNormaliza (None, None, 64, 64) 256         Conv2D_1-2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "MaxPooling2D_1 (MaxPooling2D)   (None, None, 32, 64) 0           BatchNormal_1-2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_2-1 (Conv2D)             (None, None, 32, 128 73856       MaxPooling2D_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_2-1 (BatchNormaliza (None, None, 32, 128 512         Conv2D_2-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_2-2 (Conv2D)             (None, None, 32, 128 147584      BatchNormal_2-1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_2-2 (BatchNormaliza (None, None, 32, 128 512         Conv2D_2-2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "MaxPooling2D_2 (MaxPooling2D)   (None, None, 16, 128 0           BatchNormal_2-2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_3-1 (Conv2D)             (None, None, 16, 256 295168      MaxPooling2D_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_3-1 (BatchNormaliza (None, None, 16, 256 1024        Conv2D_3-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_3-2 (Conv2D)             (None, None, 16, 256 590080      BatchNormal_3-1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_3-2 (BatchNormaliza (None, None, 16, 256 1024        Conv2D_3-2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "MaxPooling2D_3 (MaxPooling2D)   (None, None, 8, 256) 0           BatchNormal_3-2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_4-1 (Conv2D)             (None, None, 8, 512) 1180160     MaxPooling2D_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_4-1 (BatchNormaliza (None, None, 8, 512) 2048        Conv2D_4-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_4-2 (Conv2D)             (None, None, 8, 512) 2359808     BatchNormal_4-1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_4-2 (BatchNormaliza (None, None, 8, 512) 2048        Conv2D_4-2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Reshape_1 (Reshape)             (None, None, 4096)   0           BatchNormal_4-2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (LSTM)                    (None, None, 128)    2163200     Reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (LSTM)                    (None, None, 256)    394240      lstm1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Dense_1 (Dense)                 (None, None, 512)    131584      lstm2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Dense_2 (Dense)                 (None, None, 11)     5643        Dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Activation_1 (Activation)       (None, None, 11)     0           Dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           the_labels[0][0]                 \n",
      "                                                                 Activation_1[0][0]               \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,386,571\n",
      "Trainable params: 7,382,731\n",
      "Non-trainable params: 3,840\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "image_size=64  \n",
    "batch_size = 16\n",
    "logs_path = '../logs'\n",
    "\n",
    "input_size=(None, image_size, 1)\n",
    "base_model, ctc_model = create_model(input_size, output_size=py_list_size) \n",
    "print(ctc_model.summary())\n",
    "\n",
    "# keras.utils.plot_model(base_model, show_shapes=True)\n",
    "\n",
    "if not os.path.exists(logs_path):  # 判断保存模型的目录是否存在\n",
    "    os.makedirs(logs_path)  # 如果不存在，就新建一个，避免之后保存模型的时候炸掉\n",
    "\n",
    "# train_batch_gen = get_batch_generator(batch_size, train_wav_file_list, train_label_list, py_list, image_size)\n",
    "# validate_batch_gen = get_batch_generator(batch_size, validate_wav_file_list, validate_label_list, py_list, image_size)\n",
    "# input_data = next(train_batch_gen)[0]\n",
    "# plt.imshow(input_data['the_inputs'][0].T[0])\n",
    "# plt.show()\n",
    "# print(input_data['the_inputs'].shape, input_data['the_labels'].shape, input_data['input_length'].shape, input_data['label_length'].shape)\n",
    "\n",
    "train_data = get_train_data(train_wav_file_list, train_label_list, py_list, image_size)\n",
    "\n",
    "cb = []\n",
    "cb.append(keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "# 当监测值不再改善时，该回调函数将中止训练可防止过拟合\n",
    "cb.append(keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=50, verbose=1, mode='auto'))\n",
    "# his = ctc_model.fit_generator(train_batch_gen, verbose=1, steps_per_epoch=train_file_nums//batch_size, validation_data=validate_batch_gen, validation_steps=validate_file_nums//batch_size, epochs=1000, callbacks=cb)  \n",
    "\n",
    "his = ctc_model.fit(train_data[0], train_data[1], validation_split=0.1, batch_size=128, epochs=1000, callbacks=cb)  # callback的epoch都是对fit里的参数来说\n",
    "\n",
    "#  保存模型结构及权重\n",
    "ctc_model.save_weights(r'save_weights.h5')\n",
    "with open(r'model_struct.json', 'w') as f:\n",
    "    json_string = base_model.to_json()\n",
    "    f.write(json_string)  # 保存模型信息\n",
    "print('模型结构及权重已保存')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "91F-ESvt91QL",
    "outputId": "15d77e50-ff58-46a0-9f97-4447c441babd"
   },
   "outputs": [],
   "source": [
    "# 加载权重\n",
    "with open(r'model_struct.json') as f:\n",
    "    model_struct = f.read()\n",
    "test_model = keras.models.model_from_json(model_struct)\n",
    "test_model.load_weights(r'save_weights.h5')\n",
    "# model = keras.models.load_model('all_model.h5')\n",
    "print('模型已加载')\n",
    "py_list = []\n",
    "with open(r'pinyin_list.txt', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "for line in contents:\n",
    "    i = line.strip('\\n')\n",
    "    py_list.append(i)\n",
    "print('py_list已加载')\n",
    "\n",
    "# 对模型进行评价\n",
    "\n",
    "\n",
    "def evaluate(kind, wavs, labels):\n",
    "    data_num = len(wavs)\n",
    "    error_cnt = 0\n",
    "    for i in range(data_num):\n",
    "        pre = test_model.predict(wavs[i])  # (1, 20, 11)\n",
    "        pre_index, pre_label = decode_ctc(pre, py_list)  # ['5']\n",
    "        try:\n",
    "            pre_label = int(pre_label[0])\n",
    "        except:\n",
    "            pre_label = None\n",
    "        label = int(py_list[labels[i]])\n",
    "        if label != pre_label:\n",
    "            error_cnt += 1\n",
    "            print('真实标签：', label, '预测结果', pre_label)\n",
    "    print('{}:样本数{}错误数{}准确率：{:%}'.format(\n",
    "        kind, data_num, error_cnt, (1-error_cnt/data_num)))\n",
    "\n",
    "\n",
    "# 训练集\n",
    "train_wavs, train_labels = get_test_data(image_size, train_wav_file_list, train_label_list, py_list)\n",
    "# 测试集\n",
    "test_wavs, test_labels = get_test_data(image_size, test_wav_file_list, test_label_list, py_list)\n",
    "\n",
    "evaluate('trian', train_wavs, train_labels)\n",
    "evaluate('test', test_wavs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0lXiT7eJVVv"
   },
   "source": [
    "#  对比实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jj9RaRpj96lC"
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "dKaLi23oJVtl",
    "outputId": "2bd0c614-4eb9-4b74-c3cb-a298083cf871"
   },
   "outputs": [],
   "source": [
    "def create_cnn_model(input_size, output_size):\n",
    "    inputs = Input(name='the_inputs', shape=input_size)\n",
    "    # 1\n",
    "    h1_1 = Conv2D(64, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_1-1')(inputs)\n",
    "    h1_2 = BatchNormalization(name='BatchNormal_1-1')(h1_1)\n",
    "    h1_3 = Conv2D(64, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_1-2')(h1_2)\n",
    "    h1_4 = BatchNormalization(name='BatchNormal_1-2')(h1_3)\n",
    "    h1_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_1')(h1_4)\n",
    "    # 2\n",
    "    h2_1 = Conv2D(128, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_2-1')(h1_5)\n",
    "    h2_2 = BatchNormalization(name='BatchNormal_2-1')(h2_1)\n",
    "    h2_3 = Conv2D(128, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_2-2')(h2_2)\n",
    "    h2_4 = BatchNormalization(name='BatchNormal_2-2')(h2_3)\n",
    "    h2_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_2')(h2_4)\n",
    "    # 3\n",
    "    h3_1 = Conv2D(256, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_3-1')(h2_5)\n",
    "    h3_2 = BatchNormalization(name='BatchNormal_3-1')(h3_1)\n",
    "    h3_3 = Conv2D(256, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_3-2')(h3_2)\n",
    "    h3_4 = BatchNormalization(name='BatchNormal_3-2')(h3_3)\n",
    "    h3_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_3')(h3_4)\n",
    "    # 4\n",
    "    h4_1 = Conv2D(521, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_4-1')(h3_5)\n",
    "    h4_2 = BatchNormalization(name='BatchNormal_4-1')(h4_1)\n",
    "    h4_3 = Conv2D(512, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_4-2')(h4_2)\n",
    "    h4_4 = BatchNormalization(name='BatchNormal_4-2')(h4_3)\n",
    "    # 由于声学模型网络结构原因（3个maxpooling层），我们的音频数据的每个维度需要能够被8整除。这里输入序列经过卷积网络后，长度缩短了8倍，因此我们训练实际输入的数据为wav_len//8。\n",
    "    h5_1 = Reshape((-1, int(input_size[1]//8*512)), name='Reshape_1')(h4_4)\n",
    "    h5_2 = Dense(512, activation='relu', use_bias=True, kernel_initializer='he_normal', name='Dense_1')(h5_1)\n",
    "    h5_3 = Dense(output_size, activation=\"relu\", use_bias=True, kernel_initializer='he_normal', name='Dense_2')(h5_2)#(layer_h15)\n",
    "\n",
    "    outputs = Activation('softmax', name='Activation_1')(h5_3)\n",
    "\n",
    "    base_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    labels = Input(name='the_labels', shape=[None], dtype='float32')\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "    # keras.Lambda(function, output_shape=None, mask=None, arguments=None)\n",
    "    # 将任意表达式封装为 Layer 对象\n",
    "    loss_out = Lambda(ctc_lambda, output_shape=(1,), name='ctc')([labels, outputs, input_length, label_length])\n",
    "    ctc_model = keras.Model(inputs=[labels, inputs, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.0008, beta_1=0.9, beta_2=0.999, decay=0.01, epsilon=10e-8)\n",
    "    # ctc_model=multi_gpu_model(ctc_model,gpus=2)\n",
    "    ctc_model.compile(loss={'ctc': lambda y_true, output: output}, optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return base_model, ctc_model\n",
    "base_cnn_model, ctc_cnn_model = create_cnn_model(input_size, output_size=py_list_size) \n",
    "\n",
    "his = ctc_cnn_model.fit(train_data[0], train_data[1], validation_split=0.1, batch_size=32, epochs=1000, callbacks=cb)  # callback的epoch都是对fit里的参数来说\n",
    "\n",
    "#  保存模型结构及权重\n",
    "ctc_cnn_model.save_weights(r'save_cnn_weights.h5')\n",
    "with open(r'model_cnn_struct.json', 'w') as f:\n",
    "    json_string = base_cnn_model.to_json()\n",
    "    f.write(json_string)  # 保存模型信息\n",
    "print('模型结构及权重已保存')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "uWtp6s_89_WL",
    "outputId": "d00daf27-3ec8-4bfb-95ce-696f1757a2dc"
   },
   "outputs": [],
   "source": [
    "# 加载权重\n",
    "with open(r'model_cnn_struct.json') as f:\n",
    "    model_cnn_struct = f.read()\n",
    "test_cnn_model = keras.models.model_from_json(model_cnn_struct)\n",
    "# model.summary()\n",
    "test_cnn_model.load_weights(r'save_cnn_weights.h5')\n",
    "# model = keras.models.load_model('all_model.h5')\n",
    "print('模型已加载')\n",
    "py_list = []\n",
    "with open(r'pinyin_list.txt', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "for line in contents:\n",
    "    i = line.strip('\\n')\n",
    "    py_list.append(i)\n",
    "print('py_list已加载')\n",
    "\n",
    "# 对模型进行评价\n",
    "\n",
    "\n",
    "def evaluate(kind, wavs, labels):\n",
    "    data_num = len(wavs)\n",
    "    error_cnt = 0\n",
    "    for i in range(data_num):\n",
    "        pre = test_cnn_model.predict(wavs[i])  # (1, 20, 11)\n",
    "        pre_index, pre_label = decode_ctc(pre, py_list)  # ['5']\n",
    "        try:\n",
    "            pre_label = int(pre_label[0])\n",
    "        except:\n",
    "            pre_label = None\n",
    "        label = int(py_list[labels[i]])\n",
    "        if label != pre_label:\n",
    "            error_cnt += 1\n",
    "            print('真实标签：', label, '预测结果', pre_label)\n",
    "    print('{}:样本数{}错误数{}准确率：{:%}'.format(\n",
    "        kind, data_num, error_cnt, (1-error_cnt/data_num)))\n",
    "\n",
    "evaluate('trian', train_wavs, train_labels)\n",
    "evaluate('test', test_wavs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zescE9wJ-BtP"
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8zL0RHpIjEJk"
   },
   "outputs": [],
   "source": [
    "# #LSTM minist分类\n",
    "# from keras.datasets import mnist\n",
    "# n_input = 28\n",
    "# n_step = 28\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# x_train = x_train.reshape(-1, n_step, n_input, 1)\n",
    "# x_test = x_test.reshape(-1, n_step, n_input, 1)\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "# y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "# inputs = Input(name='the_inputs', shape=(n_step, n_input, 1))\n",
    "# inner = Reshape((n_step, n_input),name='Reshape_1')(inputs)\n",
    "# # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "# lstm_1 = LSTM(128, kernel_initializer='he_normal', name='lstm1')(inner)\n",
    "# d1 = Dense(10)(lstm_1)\n",
    "# outputs = Activation('softmax')(d1)\n",
    "# model = keras.Model(inputs, outputs)\n",
    "\n",
    "# model.summary()\n",
    "# model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# model.fit(x_train, y_train,batch_size=128,epochs=20,verbose=1,validation_data=(x_test, y_test))\n",
    "# scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print('LSTM test score:', scores[0])\n",
    "# print('LSTM test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "hF_MVb2fIlbx",
    "outputId": "57000e60-0043-416a-cc49-e1607b1f4e57"
   },
   "outputs": [],
   "source": [
    "# LSTM太差\n",
    "def create_lstm_model(input_size, output_size):\n",
    "    inputs = Input(name='the_inputs', shape=input_size)\n",
    "    inner = Reshape((72, 64), name='Reshape_1')(inputs)\n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    lstm_1 = Bidirectional(LSTM(32, return_sequences=True,\n",
    "                                kernel_initializer='he_normal', name='lstm1'))(inner)\n",
    "\n",
    "    lstm_2 = Bidirectional(LSTM(128, return_sequences=True,\n",
    "                                kernel_initializer='he_normal', name='lstm3'))(lstm_1)\n",
    "\n",
    "    h5_2 = Dense(512, activation='relu', use_bias=True,\n",
    "                 kernel_initializer='he_normal', name='Dense_4')(lstm_2)\n",
    "    h5_3 = Dense(output_size, activation=\"relu\", use_bias=True,\n",
    "                 kernel_initializer='he_normal', name='Dense_5')(h5_2)  # (layer_h15)\n",
    "\n",
    "    outputs = Activation('softmax', name='Activation_1')(h5_3)\n",
    "\n",
    "    base_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    labels = Input(name='the_labels', shape=[None], dtype='float32')\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "    # keras.Lambda(function, output_shape=None, mask=None, arguments=None)\n",
    "    # 将任意表达式封装为 Layer 对象\n",
    "    loss_out = Lambda(ctc_lambda, output_shape=(1,), name='ctc')(\n",
    "        [labels, outputs, input_length, label_length])\n",
    "    ctc_model = keras.Model(\n",
    "        inputs=[labels, inputs, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "    opt = keras.optimizers.Adam(\n",
    "        lr=0.0008, beta_1=0.9, beta_2=0.999, decay=0.01, epsilon=10e-8)\n",
    "    # ctc_model=multi_gpu_model(ctc_model,gpus=2)\n",
    "    ctc_model.compile(loss={'ctc': lambda y_true, output: output},\n",
    "                      optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return base_model, ctc_model\n",
    "\n",
    "\n",
    "base_lstm_model, ctc_lstm_model = create_lstm_model(\n",
    "    input_size=(72, 64, 1), output_size=py_list_size)\n",
    "# ctc_lstm_model.summary()\n",
    "train_data = get_train_data(\n",
    "    train_wav_file_list, train_label_list, py_list, image_size)\n",
    "his = ctc_lstm_model.fit(train_data[0], train_data[1], validation_split=0.1,\n",
    "                         batch_size=32, verbose=0, epochs=1000, callbacks=cb)  # callback的epoch都是对fit里的参数来说\n",
    "\n",
    "#  保存模型结构及权重\n",
    "ctc_lstm_model.save_weights(r'save_lstm_weights.h5')\n",
    "with open(r'model_lstm_struct.json', 'w') as f:\n",
    "    json_string = base_lstm_model.to_json()\n",
    "    f.write(json_string)  # 保存模型信息\n",
    "print('模型结构及权重已保存')\n",
    "\n",
    "# 加载权重\n",
    "with open(r'model_lstm_struct.json') as f:\n",
    "    model_lstm_struct = f.read()\n",
    "test_lstm_model = keras.models.model_from_json(model_lstm_struct)\n",
    "\n",
    "test_lstm_model.load_weights(r'save_lstm_weights.h5')\n",
    "print('模型已加载')\n",
    "py_list = []\n",
    "with open(r'pinyin_list.txt', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "for line in contents:\n",
    "    i = line.strip('\\n')\n",
    "    py_list.append(i)\n",
    "print('py_list已加载')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AstalecH-Rod",
    "outputId": "357d6ae8-9a68-4ed5-f52b-cbfb24887f74"
   },
   "outputs": [],
   "source": [
    "# 对模型进行评价\n",
    "def evaluate(kind, wavs, labels):\n",
    "    data_num = len(wavs)\n",
    "    error_cnt = 0\n",
    "    for i in range(data_num):\n",
    "        pre = test_lstm_model.predict(wavs[i])  # (1, 20, 11)\n",
    "        pre_index, pre_label = decode_ctc(pre, py_list)  # ['5']\n",
    "        try:\n",
    "            pre_label = int(pre_label[0])\n",
    "        except:\n",
    "            pre_label = None\n",
    "        label = int(py_list[labels[i]])\n",
    "        if label != pre_label:\n",
    "            error_cnt += 1\n",
    "            print('真实标签：', label, '预测结果', pre_label)\n",
    "    print('{}:样本数{}错误数{}准确率：{:%}'.format(\n",
    "        kind, data_num, error_cnt, (1-error_cnt/data_num)))\n",
    "\n",
    "\n",
    "# 训练集\n",
    "train_wavs, train_labels = get_test_data(\n",
    "    image_size, train_wav_file_list, train_label_list, py_list)\n",
    "# 测试集\n",
    "test_wavs, test_labels = get_test_data(\n",
    "    image_size, test_wav_file_list, test_label_list, py_list)\n",
    "\n",
    "evaluate('trian', train_wavs, train_labels)\n",
    "evaluate('test', test_wavs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJen5_JX-UOn"
   },
   "source": [
    "## 交叉熵损失函数的CRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "NZd7lxJiJ7Av",
    "outputId": "dafd9604-2c5b-487e-f555-fdd181e174c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00030: early stopping\n",
      "模型结构及权重已保存\n"
     ]
    }
   ],
   "source": [
    "def create_cross_model(input_size, output_size):\n",
    "    inputs = Input(name='the_inputs', shape=input_size)\n",
    "    # 1\n",
    "    h1_1 = Conv2D(64, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_1-1')(inputs)\n",
    "    h1_2 = BatchNormalization(name='BatchNormal_1-1')(h1_1)\n",
    "    h1_3 = Conv2D(64, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_1-2')(h1_2)\n",
    "    h1_4 = BatchNormalization(name='BatchNormal_1-2')(h1_3)\n",
    "    h1_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_1')(h1_4)\n",
    "    # 2\n",
    "    h2_1 = Conv2D(128, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_2-1')(h1_5)\n",
    "    h2_2 = BatchNormalization(name='BatchNormal_2-1')(h2_1)\n",
    "    h2_3 = Conv2D(128, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_2-2')(h2_2)\n",
    "    h2_4 = BatchNormalization(name='BatchNormal_2-2')(h2_3)\n",
    "    h2_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_2')(h2_4)\n",
    "    # 3\n",
    "    h3_1 = Conv2D(256, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_3-1')(h2_5)\n",
    "    h3_2 = BatchNormalization(name='BatchNormal_3-1')(h3_1)\n",
    "    h3_3 = Conv2D(256, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_3-2')(h3_2)\n",
    "    h3_4 = BatchNormalization(name='BatchNormal_3-2')(h3_3)\n",
    "    h3_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_3')(h3_4)\n",
    "    # 4\n",
    "    h4_1 = Conv2D(512, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_4-1')(h3_5)\n",
    "    h4_2 = BatchNormalization(name='BatchNormal_4-1')(h4_1)\n",
    "    h4_3 = Conv2D(512, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_4-2')(h4_2)\n",
    "    h4_4 = BatchNormalization(name='BatchNormal_4-2')(h4_3)\n",
    "    # 由于声学模型网络结构原因（3个maxpooling层），我们的音频数据的每个维度需要能够被8整除。这里输入序列经过卷积网络后，长度缩短了8倍，因此我们训练实际输入的数据为wav_len//8。\n",
    "    h5_1 = Reshape((9, int(input_size[1]//8*512)), name='Reshape_1')(h4_4)\n",
    "\n",
    "    lstm_1 = LSTM(128, return_sequences=True, kernel_initializer='he_normal', name='lstm1')(h5_1)\n",
    "    lstm_2 = LSTM(256, return_sequences=True, kernel_initializer='he_normal', name='lstm2')(lstm_1)\n",
    "    h5_11 = Flatten()(lstm_2)\n",
    "    h5_2 = Dense(512, activation='relu', use_bias=True, kernel_initializer='he_normal', name='Dense_1')(h5_11)\n",
    "    h5_3 = Dense(output_size, activation=\"relu\", use_bias=True, kernel_initializer='he_normal', name='Dense_2')(h5_2)#(layer_h15)\n",
    "    outputs = Activation('softmax', name='Activation_1')(h5_3)\n",
    "    base_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.09, beta_2=0.999, decay=0.1, epsilon=10e-8)\n",
    "    # ctc_model=multi_gpu_model(ctc_model,gpus=2)\n",
    "    base_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return base_model\n",
    "\n",
    "crnn_cross_model = create_cross_model(input_size=(72, 64, 1), output_size=6) \n",
    "\n",
    "train_data = get_train_data(train_wav_file_list, train_label_list, py_list, image_size) \n",
    "from keras.utils import np_utils\n",
    "\n",
    "l = np_utils.to_categorical(train_data[0]['the_labels'], num_classes=6)\n",
    "his = crnn_cross_model.fit(train_data[0]['the_inputs'], l, validation_split=0.2, batch_size=32, verbose=0, epochs=1000, callbacks=cb)  # callback的epoch都是对fit里的参数来说\n",
    "\n",
    "#  保存模型结构及权重\n",
    "crnn_cross_model.save_weights(r'save_crnn_cross_weights.h5')\n",
    "with open(r'model_crnn_cross_struct.json', 'w') as f:\n",
    "    json_string = crnn_cross_model.to_json()\n",
    "    f.write(json_string)  # 保存模型信息\n",
    "print('模型结构及权重已保存')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "QEJf_dDx-bNA",
    "outputId": "695075fd-976c-438c-c3c5-2ef1c9630f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已加载\n",
      "py_list已加载\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 184320 into shape (60,72,64,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-0dd5571531bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpy_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'py_list已加载'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrnn_cross_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_wavs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m72\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'准确率：{:%}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 184320 into shape (60,72,64,1)"
     ]
    }
   ],
   "source": [
    "# 加载权重\n",
    "with open(r'model_crnn_cross_struct.json') as f:\n",
    "  crnn_cross_struct = f.read()\n",
    "test_crnn_cross_model = keras.models.model_from_json(crnn_cross_struct)\n",
    "test_crnn_cross_model.load_weights(r'save_crnn_cross_weights.h5')\n",
    "print('模型已加载')\n",
    "py_list = []\n",
    "with open(r'pinyin_list.txt', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "for line in contents:\n",
    "    i = line.strip('\\n')\n",
    "    py_list.append(i)\n",
    "print('py_list已加载')\n",
    "loss, accuracy = crnn_cross_model.evaluate(np.array(test_wavs).reshape((60, 72, 64, 1)), np_utils.to_categorical(np.array(test_labels),num_classes=6))\n",
    "\n",
    "print('准确率：{:%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AyHxP5DpwGje"
   },
   "outputs": [],
   "source": [
    "# shown_offset = re.findall(re.compile(r'<ul.*?id=\"feedlist_id\" shown-offset=\"(.*?)\">', re.S), response.text)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OD_65MloWC7L"
   },
   "source": [
    "## 选择方差第二大的子载波"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XjMy46Gcora2",
    "outputId": "3d176941-de39-4166-debb-83dabd02c41d"
   },
   "outputs": [],
   "source": [
    "def process_data(raw_file_path, save_path):\n",
    "    X = {}\n",
    "    for f in tqdm(os.listdir(raw_file_path)):\n",
    "        if f.endswith('.dat'):\n",
    "            file_name = os.path.join(raw_file_path, f)\n",
    "            extracted_data = extract_csi(file_name)\n",
    "            print('processing {} the length of this file is:{}'.format(\n",
    "                file_name, len(extracted_data)))\n",
    "            tx, rx, sub = extracted_data[0]['csi'].shape\n",
    "            data_csi = np.zeros(\n",
    "                (len(extracted_data), tx, rx, sub), dtype=np.complex64)\n",
    "            for i in range(len(extracted_data)):\n",
    "                data_csi[i] = get_scaled_csi(extracted_data[i])\n",
    "            data_csi = np.clip(np.abs(np.squeeze(data_csi)), 1e-8,\n",
    "                               1e100)[:, :, :2, :].reshape(-1, 120)   # (205, 2, 2, 30)\n",
    "            data = np.zeros((data_csi.shape[0], 120))\n",
    "            var = []\n",
    "            for i in range(120):\n",
    "                data_csi_sub = data_csi[:, i]\n",
    "                b, a = signal.butter(5, 4*2/30, 'low')\n",
    "                carrier_data = signal.lfilter(b, a, data_csi_sub)  # N*1\n",
    "                data[:, i] = carrier_data\n",
    "\n",
    "                # length = len(carrier_data)\n",
    "                # var_temp = np.var(carrier_data[length//5:3*length//5])\n",
    "                # var.append(var_temp)\n",
    "\n",
    "            #data = data_csi[:, np.argsort(var)[1]]\n",
    "            # print(np.argsort(var)[1])\n",
    "            scio.savemat(os.path.join(\n",
    "                save_path, f.split('.')[0]+'.mat'), {'csi': data})\n",
    "            X[f] = data\n",
    "    print('all raw file processed')\n",
    "    return X\n",
    "\n",
    "\n",
    "train_raw_file_path = r'/content/drive/My Drive/SR_CSI/Gestures_data/train'\n",
    "train_save_path = r'/content/drive/My Drive/SR_CSI/Gestures_data/train'\n",
    "X_train = process_data(train_raw_file_path, train_save_path)\n",
    "test_raw_file_path = r'/content/drive/My Drive/SR_CSI/Gestures_data/test'\n",
    "test_save_path = r'/content/drive/My Drive/SR_CSI//Gestures_data/test'\n",
    "X_test = process_data(test_raw_file_path, test_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "pw0BFfg92Vxv",
    "outputId": "1da292ff-4b7a-4eec-fa03-f1e6ef42a758"
   },
   "outputs": [],
   "source": [
    "# 重新构造mat文件选择最佳子载波上的CSI\n",
    "best_index_list = [58, 1, 29, 29, 28, 29]\n",
    "\n",
    "#\n",
    "for f in tqdm(os.listdir(train_save_path)):\n",
    "    if f.endswith('.mat'):\n",
    "      file_name = os.path.join(train_save_path, f)\n",
    "      activate_index = int(re.findall(re.compile(r'csi-s1-e1-a(.*?)-.*?.mat', re.S), f)[0])\n",
    "      csi = np.squeeze(scio.loadmat(file_name)['csi'])\n",
    "      best_csi = csi[:, best_index_list[activate_index-1]]     \n",
    "      scio.savemat(file_name, {'csi': best_csi})\n",
    "for f in tqdm(os.listdir(test_save_path)):\n",
    "    if f.endswith('.mat'):\n",
    "      file_name = os.path.join(test_save_path, f)\n",
    "      activate_index = int(re.findall(re.compile(r'csi-s1-e1-a(.*?)-.*?.mat', re.S), f)[0])\n",
    "      best_csi = np.squeeze(scio.loadmat(file_name)['csi'])[:, best_index_list[activate_index-1]]\n",
    "      scio.savemat(file_name, {'csi': best_csi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "YebtHSJ4VBn8",
    "outputId": "08535eb4-e3aa-4cba-dfb0-cb5bc70fbd84"
   },
   "outputs": [],
   "source": [
    "train_file_path = train_save_path\n",
    "train_label_file_list, train_wav_file_list = get_file_list(train_file_path)\n",
    "\n",
    "train_label_list = get_label_data(train_label_file_list)\n",
    "\n",
    "# 用迭代器的时候需要\n",
    "# train_wav_file_list, validate_wav_file_list, train_label_list, validate_label_list = train_test_split(train_wav_file_list, train_label_list, test_size=0.2, random_state=0)\n",
    "\n",
    "# 每个文件拼音标签的集合\n",
    "with open('label_list.txt', 'w') as f:  \n",
    "    f.write('\\n'.join(train_label_list))  \n",
    "py_list = gen_py_list(train_label_list)  # 拼音的集合\n",
    "\n",
    "with open('pinyin_list.txt', 'w') as f:\n",
    "    f.write('\\n'.join(py_list))  # 保存拼音列表\n",
    "\n",
    "train_file_nums = len(train_wav_file_list)\n",
    "# validate_file_nums = len(validate_wav_file_list)\n",
    "\n",
    "py_list_size = len(py_list)  # 模型输出的维度\n",
    "print('py_list:', py_list)\n",
    "print('train files amount:', len(train_label_list), train_file_nums, 'label amount:', py_list_size)#, 'validate files amount:', len(validate_label_list), validate_file_nums,)\n",
    "\n",
    "# 测试数据\n",
    "test_file_path = test_save_path # r'/content/drive/My Drive/data_thchs30'\n",
    "test_label_file_list, test_wav_file_list = get_file_list(test_file_path)\n",
    "\n",
    "test_label_list = get_label_data(test_label_file_list)\n",
    "print('test files amount:',len(test_label_file_list), len(test_wav_file_list))\n",
    "test_data_num = len(test_label_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "R0vC4Ji3Wdbf",
    "outputId": "96cdb91b-9c1d-4bd4-ee57-c62d4c4fdfec"
   },
   "outputs": [],
   "source": [
    "base_sec_model, ctc_sec_model = create_model(input_size, output_size=py_list_size) \n",
    "\n",
    "train_data = get_train_data(train_wav_file_list, train_label_list, py_list, image_size)\n",
    "\n",
    "\n",
    "cb = []\n",
    "cb.append(keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "# 当监测值不再改善时，该回调函数将中止训练可防止过拟合\n",
    "cb.append(keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, verbose=1, mode='auto'))\n",
    "# his = ctc_model.fit_generator(train_batch_gen, verbose=1, steps_per_epoch=train_file_nums//batch_size, validation_data=validate_batch_gen, validation_steps=validate_file_nums//batch_size, epochs=1000, callbacks=cb)  \n",
    "\n",
    "\n",
    "his = ctc_sec_model.fit(train_data[0], train_data[1], validation_split=0.1, batch_size=32, verbose=0, epochs=1000, callbacks=cb)  # callback的epoch都是对fit里的参数来说\n",
    "\n",
    "#  保存模型结构及权重\n",
    "ctc_sec_model.save_weights(r'save_sec_weights.h5')\n",
    "with open(r'model_sec_struct.json', 'w') as f:\n",
    "    json_string = base_sec_model.to_json()\n",
    "    f.write(json_string)  # 保存模型信息\n",
    "print('模型结构及权重已保存')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GYzx9TuLWZDV",
    "outputId": "8d9ce035-e20a-4b69-b97f-e2e41f8c86e5"
   },
   "outputs": [],
   "source": [
    "# 加载权重\n",
    "with open(r'model_sec_struct.json') as f:\n",
    "  model_struct = f.read()\n",
    "test_sec_model = keras.models.model_from_json(model_struct)\n",
    "test_sec_model.load_weights(r'save_sec_weights.h5')\n",
    "# model = keras.models.load_model('all_model.h5')\n",
    "print('模型已加载')\n",
    "py_list = []\n",
    "with open(r'pinyin_list.txt', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "for line in contents:\n",
    "    i = line.strip('\\n')\n",
    "    py_list.append(i)\n",
    "print('py_list已加载')\n",
    "\n",
    "# 对模型进行评价\n",
    "def evaluate(kind, wavs, labels):\n",
    "  data_num = len(wavs)\n",
    "  error_cnt = 0\n",
    "  for i in range(data_num):\n",
    "    pre = test_sec_model.predict(wavs[i]) # (1, 20, 11)\n",
    "    pre_index, pre_label = decode_ctc(pre, py_list) # ['5']\n",
    "    try:\n",
    "      pre_label = int(pre_label[0])\n",
    "    except:\n",
    "      pre_label = None\n",
    "    label = int(py_list[labels[i]])\n",
    "    if label != pre_label:\n",
    "      error_cnt += 1\n",
    "      print('真实标签：', label, '预测结果', pre_label)\n",
    "  print('{}:样本数{}错误数{}准确率：{:%}'.format(kind, data_num, error_cnt, (1-error_cnt/data_num)))\n",
    "\n",
    "# 训练集\n",
    "train_wavs, train_labels = get_test_data(image_size, train_wav_file_list, train_label_list, py_list)\n",
    "# 测试集\n",
    "test_wavs, test_labels = get_test_data(image_size, test_wav_file_list, test_label_list, py_list)\n",
    "\n",
    "evaluate('trian', train_wavs, train_labels)\n",
    "evaluate('test', test_wavs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTFVTpBCXNs1"
   },
   "source": [
    "## 所有子载波求平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cf4NJzVoTw94",
    "outputId": "dcb07260-fb9a-4be4-80e9-ce8ae7d0410b"
   },
   "outputs": [],
   "source": [
    "def process_data(raw_file_path, save_path):\n",
    "  X={}\n",
    "  for f in tqdm(os.listdir(raw_file_path)):\n",
    "    if f.endswith('.dat'):\n",
    "      file_name = os.path.join(raw_file_path, f)\n",
    "      extracted_data = extract_csi(file_name)\n",
    "      print('processing {} the length of this file is:{}'.format(file_name, len(extracted_data)))\n",
    "      tx, rx, sub = extracted_data[0]['csi'].shape\n",
    "      data_csi = np.zeros((len(extracted_data), tx, rx, sub), dtype=np.complex64)\n",
    "      for i in range(len(extracted_data)):\n",
    "        data_csi[i] = get_scaled_csi(extracted_data[i])\n",
    "      data_csi = np.clip(np.abs(np.squeeze(data_csi)), 1e-8, 1e100)[:,:,:2,:].reshape(-1, 4, 30)   # (205, 2, 2, 30)\n",
    "      data = []\n",
    "      for i in range(4):\n",
    "        data_ave = np.average(data_csi[:, i, :],axis=1)\n",
    "        data.extend(data_ave)\n",
    "      scio.savemat(os.path.join(save_path, f.split('.')[0]+'.mat'), {'csi': data})\n",
    "      X[f]=data\n",
    "  print('all raw file processed')\n",
    "  return X\n",
    "\n",
    "train_raw_file_path = r'/content/drive/My Drive/SR_CSI/Gestures_data/train'\n",
    "train_save_path = r'/content/drive/My Drive/SR_CSI/Gestures_data/train'\n",
    "X_train = process_data(train_raw_file_path, train_save_path)\n",
    "test_raw_file_path = r'/content/drive/My Drive/SR_CSI/Gestures_data/test'\n",
    "test_save_path = r'/content/drive/My Drive/SR_CSI//Gestures_data/test'\n",
    "X_test = process_data(test_raw_file_path, test_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "YesArgHvXc2O",
    "outputId": "712bf146-e937-4b33-9489-3906b5521aca"
   },
   "outputs": [],
   "source": [
    "train_file_path = train_save_path\n",
    "train_label_file_list, train_wav_file_list = get_file_list(train_file_path)\n",
    "\n",
    "train_label_list = get_label_data(train_label_file_list)\n",
    "\n",
    "# 用迭代器的时候需要\n",
    "# train_wav_file_list, validate_wav_file_list, train_label_list, validate_label_list = train_test_split(train_wav_file_list, train_label_list, test_size=0.2, random_state=0)\n",
    "\n",
    "# 每个文件拼音标签的集合\n",
    "with open('label_list.txt', 'w') as f:  \n",
    "    f.write('\\n'.join(train_label_list))  \n",
    "py_list = gen_py_list(train_label_list)  # 拼音的集合\n",
    "\n",
    "with open('pinyin_list.txt', 'w') as f:\n",
    "    f.write('\\n'.join(py_list))  # 保存拼音列表\n",
    "\n",
    "train_file_nums = len(train_wav_file_list)\n",
    "# validate_file_nums = len(validate_wav_file_list)\n",
    "\n",
    "py_list_size = len(py_list)  # 模型输出的维度\n",
    "print('py_list:', py_list)\n",
    "print('train files amount:', len(train_label_list), train_file_nums, 'label amount:', py_list_size)#, 'validate files amount:', len(validate_label_list), validate_file_nums,)\n",
    "\n",
    "# 测试数据\n",
    "test_file_path = test_save_path # r'/content/drive/My Drive/data_thchs30'\n",
    "test_label_file_list, test_wav_file_list = get_file_list(test_file_path)\n",
    "\n",
    "test_label_list = get_label_data(test_label_file_list)\n",
    "print('test files amount:',len(test_label_file_list), len(test_wav_file_list))\n",
    "test_data_num = len(test_label_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7P0aTudcXhWc"
   },
   "outputs": [],
   "source": [
    "base_ave_model, ctc_ave_model = create_model(input_size=(None, 256, 1), output_size=py_list_size) \n",
    "\n",
    "train_data = get_train_data(train_wav_file_list, train_label_list, py_list, image_size=256)\n",
    "\n",
    "his = ctc_ave_model.fit(train_data[0], train_data[1], validation_split=0.1, batch_size=32, verbose=1, epochs=1000, callbacks=cb)  # callback的epoch都是对fit里的参数来说\n",
    "\n",
    "#  保存模型结构及权重\n",
    "ctc_ave_model.save_weights(r'save_ave_weights.h5')\n",
    "with open(r'model_ave_struct.json', 'w') as f:\n",
    "    json_string = base_ave_model.to_json()\n",
    "    f.write(json_string)  # 保存模型信息\n",
    "print('模型结构及权重已保存')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZRuz4mZDXzBv",
    "outputId": "d41cf82a-9465-447e-beba-c1987ff10ee6"
   },
   "outputs": [],
   "source": [
    "# 加载权重\n",
    "with open(r'model_ave_struct.json') as f:\n",
    "  model_struct = f.read()\n",
    "test_ave_model = keras.models.model_from_json(model_struct)\n",
    "test_ave_model.load_weights(r'save_ave_weights.h5')\n",
    "# model = keras.models.load_model('all_model.h5')\n",
    "print('模型已加载')\n",
    "py_list = []\n",
    "with open(r'pinyin_list.txt', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "for line in contents:\n",
    "    i = line.strip('\\n')\n",
    "    py_list.append(i)\n",
    "print('py_list已加载')\n",
    "\n",
    "# 对模型进行评价\n",
    "def evaluate(kind, wavs, labels):\n",
    "  data_num = len(wavs)\n",
    "  error_cnt = 0\n",
    "  for i in range(data_num):\n",
    "    pre = test_sec_model.predict(wavs[i]) # (1, 20, 11)\n",
    "    pre_index, pre_label = decode_ctc(pre, py_list) # ['5']\n",
    "    try:\n",
    "      pre_label = int(pre_label[0])\n",
    "    except:\n",
    "      pre_label = None\n",
    "    label = int(py_list[labels[i]])\n",
    "    if label != pre_label:\n",
    "      error_cnt += 1\n",
    "      print('真实标签：', label, '预测结果', pre_label)\n",
    "  print('{}:样本数{}错误数{}准确率：{:%}'.format(kind, data_num, error_cnt, (1-error_cnt/data_num)))\n",
    "\n",
    "# 训练集\n",
    "train_wavs, train_labels = get_test_data(image_size, train_wav_file_list, train_label_list, py_list)\n",
    "# 测试集\n",
    "test_wavs, test_labels = get_test_data(image_size, test_wav_file_list, test_label_list, py_list)\n",
    "\n",
    "evaluate('trian', train_wavs, train_labels)\n",
    "evaluate('test', test_wavs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xm6T5vgpe6Ma"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X559TQs8e6NF"
   },
   "source": [
    "#### ROC、AUC曲线 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FuQricFRe6NH"
   },
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.preprocessing import label_binarize\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# import numpy as np\n",
    "# from scipy import interp\n",
    "# import matplotlib.pyplot as plt\n",
    "# from itertools import cycle\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# # 标签共三类\n",
    "# n_classes = py_list_size-1\n",
    "\n",
    "# X, y = make_classification(n_samples=80000, n_features=20, n_informative=3, n_redundant=0, n_classes=n_classes,\n",
    "#     n_clusters_per_class=2)\n",
    "# # print(X.shape, y.shape)\n",
    "# # print(X[0], y[0])\n",
    "# # (80000, 20) (80000,)\n",
    "# # [-1.90920853 -1.30052757 -0.76903467 -3.2546519  -0.02947816  0.14105006\n",
    "# #   0.43556031 -0.81300607 -0.94553296 -0.92774495  1.49041451 -0.4443121\n",
    "# #  -1.16342165 -0.32997815 -1.02907045 -0.39950447 -0.711287    0.51382424\n",
    "# #   2.88822258 -2.0935274 ] \n",
    "# # 1\n",
    "\n",
    "# # Binarize the output相当于one_hot\n",
    "# y = label_binarize(y, classes=[0, 1, 2])\n",
    "# # print(y.shape, y[0])\n",
    "# # (80000, 3) [0 1 0]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "# model = Sequential()\n",
    "# model.add(Dense(20, input_dim=20, activation='relu'))\n",
    "# model.add(Dense(40, activation='relu'))\n",
    "# model.add(Dense(3, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.fit(X_train, y_train, epochs=1, batch_size=100, verbose=1)\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "# # print(y_pred.shape)\n",
    "# # (40000, 3)\n",
    "\n",
    "# # Compute ROC curve and ROC area for each class\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "# for i in range(n_classes):\n",
    "#     # scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "#     # fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n",
    "#     # y 就是标准值，scores 是每个预测值对应的阳性概率，比如0.1就是指第一个数预测为阳性的概率为0.1，很显然，\n",
    "#     # y 和 socres应该有相同多的元素，都等于样本数。pos_label=2 是指在y中标签为2的是标准阳性标签，其余值是阴性。\n",
    "#     # 接下来选取一个阈值计算TPR/FPR,阈值的选取规则是在scores值中从大到小的以此选取，于是第一个选取的阈值是0.8\n",
    "#     # label=[1,1,2,2] scores=[0.1,0.4,0.35,0.8] thresholds=[0.8,0.4,0.35,0.1] 以threshold为0.8为例，将0.8与\n",
    "#     # scores 中所有值比较大小得到预测值，[0,0,0,1].对于label中两个1，其概率分别为0.1，0.4，小于阈值0.8，判定为\n",
    "#     # 负样本，而他们的label是1，说明他们确实是负样本，判断正确，是两个TN；两个2，对应概率为0.35，0.8，0.35小于\n",
    "#     # 0.8，判定为负样本，但是label是2，应该是个正样本，所以这是个FN；最后0.8>=0.8,这是个TP，所以最后的结果是\n",
    "#     # ：1个TP，2个TN，1个FN，0个FP\n",
    "#     fpr[i], tpr[i], thresholds = roc_curve(y_test[:, i], y_pred[:, i])  # (40000,)\n",
    "#     # print(fpr[i].shape)# (5491,)# (6562,)# (4271,)\n",
    "#     roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "\n",
    "# # 计算microROC曲线和ROC面积 \n",
    "# # .ravel()将多维数组转换为一维数组\n",
    "# fpr[\"micro\"], tpr[\"micro\"]  , thresholds = roc_curve(y_test.ravel(), y_pred.ravel())  #  (120000,)\n",
    "# roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# # 计算macroROC曲线和ROC面积\n",
    "# # 首先，汇总所有的假阳性率\n",
    "# # np.unique() 该函数是去除数组中的重复数字，并进行排序之后输出。\n",
    "# # print(np.concatenate([fpr[i] for i in range(n_classes)]).shape) (16324,)\n",
    "# all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))  # (7901,)\n",
    "# # 然后插值所有的ROC曲线在这一点\n",
    "# # np.zeros_like() 这个函数的意思就是生成一个和你所给数组a相同shape的全0数组。\n",
    "# mean_tpr = np.zeros_like(all_fpr)\n",
    "# for i in range(n_classes):\n",
    "#     mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "    \n",
    "# # 最后求平均值并计算AUC\n",
    "# mean_tpr /= n_classes\n",
    "# fpr[\"macro\"] = all_fpr\n",
    "# tpr[\"macro\"] = mean_tpr\n",
    "# roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# # Plot all ROC curves\n",
    "# plt.figure(1)\n",
    "# plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='deeppink', linestyle=':', linewidth=4,\n",
    "#          label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]))\n",
    "\n",
    "# plt.plot(fpr[\"macro\"], tpr[\"macro\"],color='navy', linestyle=':', linewidth=4,\n",
    "#          label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"macro\"]))\n",
    "\n",
    "# colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "# for i, color in zip(range(n_classes), colors):\n",
    "#     plt.plot(fpr[i], tpr[i], color=color, linewidth=2,\n",
    "#              label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Some extension of Receiver Operating Characteristic to multi-class')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # Zoom in view of the upper left corner.\n",
    "# plt.figure(2)\n",
    "# plt.xlim(0, 0.2)\n",
    "# plt.ylim(0.8, 1)\n",
    "# plt.plot(fpr[\"micro\"], tpr[\"micro\"],color='deeppink', linestyle=':', linewidth=4,\n",
    "#          label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]))\n",
    "\n",
    "# plt.plot(fpr[\"macro\"], tpr[\"macro\"],color='navy', linestyle=':', linewidth=4,\n",
    "#          label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"macro\"]))\n",
    "\n",
    "# colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "# for i, color in zip(range(n_classes), colors):\n",
    "#     plt.plot(fpr[i], tpr[i], color=color, linewidth=2,\n",
    "#              label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC curve (zoomed in at top left)')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAZqqkEue6NU"
   },
   "source": [
    "#### 混淆矩阵 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaS4QgDge6NW"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# def plot_confusion_matrix(title, y_true, y_pred, labels):\n",
    "#     cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "#     # np.newaxis的作用就是在这一位置增加一个一维，这一位置指的是np.newaxis所在的位置，比较抽象，需要配合例子理解。\n",
    "#     # x1 = np.array([1, 2, 3, 4, 5])\n",
    "#     # the shape of x1 is (5,)\n",
    "#     # x1_new = x1[:, np.newaxis]\n",
    "# # now, the shape of x1_new is (5, 1)\n",
    "\n",
    "\n",
    "#     cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#     # print (cm, '\\n\\n', cm_normalized)\n",
    "#     # [[1 0 0 0 0]                           \n",
    "#     #  [0 1 0 0 0]\n",
    "#     #  [0 0 1 0 0]\n",
    "#     #  [0 0 0 1 0]\n",
    "#     #  [0 0 0 0 1]]\n",
    "\n",
    "#     #  [[1. 0. 0. 0. 0.]\n",
    "#     #  [0. 1. 0. 0. 0.]\n",
    "#     #  [0. 0. 1. 0. 0.]\n",
    "#     #  [0. 0. 0. 1. 0.]\n",
    "#     #  [0. 0. 0. 0. 1.]]\n",
    "#     tick_marks = np.array(range(len(labels))) + 0.5\n",
    "#     #  [0.5 1.5 2.5 3.5 4.5 5.5]\n",
    "#     np.set_printoptions(precision=2)\n",
    "    \n",
    "#     plt.figure(figsize=(10, 8), dpi=120)\n",
    "#     ind_array = np.arange(len(labels))\n",
    "#     x, y = np.meshgrid(ind_array, ind_array)\n",
    "#     # print(ind_ａrray, '\\n\\n', x, '\\n\\n', y)\n",
    "#     # [0 1 2 3 4 5] \n",
    "\n",
    "#     #  [[0 1 2 3 4 5]\n",
    "#     #  [0 1 2 3 4 5]\n",
    "#     #  [0 1 2 3 4 5]\n",
    "#     #  [0 1 2 3 4 5]\n",
    "#     #  [0 1 2 3 4 5]\n",
    "#     #  [0 1 2 3 4 5]] \n",
    "\n",
    "#     #  [[0 0 0 0 0 0]\n",
    "#     #  [1 1 1 1 1 1]\n",
    "#     #  [2 2 2 2 2 2]\n",
    "#     #  [3 3 3 3 3 3]\n",
    "#     #  [4 4 4 4 4 4]\n",
    "#     #  [5 5 5 5 5 5]]\n",
    "#     intFlag = 0 # 标记在图片中对文字是整数型还是浮点型\n",
    "#     for x_val, y_val in zip(x.flatten(), y.flatten()):\n",
    "#         # plt.text()函数用于设置文字说明。\n",
    "\n",
    "#         if (intFlag):\n",
    "#             c = cm[y_val][x_val]\n",
    "#             plt.text(x_val, y_val, \"%d\" % (c,), color='red', fontsize=8, va='center', ha='center')\n",
    "\n",
    "#         else:\n",
    "#             c = cm_normalized[y_val][x_val]\n",
    "#             if (c > 0.01):\n",
    "#                 plt.text(x_val, y_val, \"%0.2f\" % (c,), color='red', fontsize=7, va='center', ha='center')\n",
    "#             else:\n",
    "#                 plt.text(x_val, y_val, \"%d\" % (0,), color='red', fontsize=7, va='center', ha='center')\n",
    "#     cmap = plt.cm.binary\n",
    "#     if(intFlag):\n",
    "#         plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     else:\n",
    "#         plt.imshow(cm_normalized, interpolation='nearest', cmap=cmap)\n",
    "#     plt.gca().set_xticks(tick_marks, minor=True)\n",
    "#     plt.gca().set_yticks(tick_marks, minor=True)\n",
    "#     plt.gca().xaxis.set_ticks_position('none')\n",
    "#     plt.gca().yaxis.set_ticks_position('none')\n",
    "#     plt.grid(True, which='minor', linestyle='-')\n",
    "#     plt.gcf().subplots_adjust(bottom=0.15)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     xlocations = np.array(range(len(labels)))\n",
    "#     plt.xticks(xlocations, labels, rotation=90)\n",
    "#     plt.yticks(xlocations, labels)\n",
    "#     plt.ylabel('Index of True Classes')\n",
    "#     plt.xlabel('Index of Predict Classes')\n",
    "#     plt.savefig('confusion_matrix.jpg', dpi=300)\n",
    "#     plt.show()\n",
    "# title='Confusion Matrix'\n",
    "# labels = ['A', 'B', 'C', 'F', 'G']\n",
    "# y_true = \n",
    "# y_pred = [1, 2, 3, 4, 5]# np.loadtxt(r'/home/dingtom/b.txt')\n",
    "# plot＿confusion_matrix(title, y_true,y_pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5T2uH7cfe6Nl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNkZObQ1e6Nr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LtKVNGRke6N-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SjJe1fnpe6OV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "keras_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tomding",
   "language": "python",
   "name": "tomding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
