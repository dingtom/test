{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "colab_type": "code",
    "id": "Vf6uB2pge6Kc",
    "outputId": "d2a3702c-2701-45d7-c0f0-0e83978cb99c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyts==0.7.1 in /home/zut_csi/anaconda3/envs/tomding/lib/python3.8/site-packages (0.7.1)\n",
      "Requirement already satisfied: numpy>=1.8.2scipy>=0.13.3scikit-learn>=0.17.0future>=0.13.1 in /home/zut_csi/anaconda3/envs/tomding/lib/python3.8/site-packages (from pyts==0.7.1) (1.19.4)\n",
      "model name\t: Intel(R) Core(TM) i9-10900F CPU @ 2.80GHz\n",
      "Fri Dec 18 21:16:43 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3070    Off  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   43C    P8    19W / 270W |    298MiB /  7979MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       451      G   ...nlogin/bin/sunloginclient        7MiB |\n",
      "|    0   N/A  N/A      1154      G   /usr/lib/xorg/Xorg                 24MiB |\n",
      "|    0   N/A  N/A      1203      G   /usr/bin/gnome-shell               53MiB |\n",
      "|    0   N/A  N/A      1505      G   /usr/lib/xorg/Xorg                141MiB |\n",
      "|    0   N/A  N/A      1633      G   /usr/bin/gnome-shell               37MiB |\n",
      "|    0   N/A  N/A      1657      G   ...mviewer/tv_bin/TeamViewer       15MiB |\n",
      "|    0   N/A  N/A      5237      G   /usr/lib/firefox/firefox            3MiB |\n",
      "|    0   N/A  N/A      5278      G   /usr/lib/firefox/firefox            3MiB |\n",
      "|    0   N/A  N/A      9636      G   /usr/lib/firefox/firefox            3MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from keras.utils import plot_model\n",
    "from struct import unpack, pack\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import scipy.io as scio\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import interpolate\n",
    "import scipy.signal as signal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "from tqdm import tqdm\n",
    "import scipy.io.wavfile as wav\n",
    "from scipy.fftpack import fft\n",
    "from random import shuffle\n",
    "from collections import Counter  \n",
    "from sklearn.ensemble import IsolationForest \n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "!pip install pyts==0.7.1\n",
    "from pyts.image import *\n",
    "try:\n",
    "    !cat /proc/cpuinfo | grep 'model name' |uniq\n",
    "    !nvidia-smi\n",
    "except:\n",
    "    print('there is no GPU') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wazutkf7xKeN"
   },
   "source": [
    "# 从.data提取CSI幅值的相关函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "W6ot9pnxW2cD"
   },
   "outputs": [],
   "source": [
    "# 从.dat文件中提取csi\n",
    "def dbinv(x):\n",
    "    return 10**(x / 10)\n",
    "# 计算接收到的信号强度(RSS)(以dBm为单位)\n",
    "def total_rss(data):\n",
    "    rssi_mag = 0\n",
    "    if data['rssi_a'] != 0:\n",
    "        rssi_mag = rssi_mag + dbinv(data['rssi_a'])\n",
    "    if data['rssi_b'] != 0:\n",
    "        rssi_mag = rssi_mag + dbinv(data['rssi_b'])\n",
    "    if data['rssi_c'] != 0:\n",
    "        rssi_mag = rssi_mag + dbinv(data['rssi_c'])\n",
    "    return 10 * np.log10(rssi_mag) - 44 - data['agc']\n",
    "# 将CSI结构转换为信道矩阵H。\n",
    "def get_scaled_csi(data):\n",
    "    csi = data['csi']\n",
    "    ntx = data['ntx']\n",
    "    nrx = data['nrx']\n",
    "    csi_sq = csi * np.conj(csi)\n",
    "    csi_pwr = csi_sq.sum().real  # 求和\n",
    "    rssi_pwr = dbinv(total_rss(data))\n",
    "    scale = rssi_pwr / (csi_pwr / 30)\n",
    "    if data['noise'] == -127:\n",
    "        noise = -92\n",
    "    else:\n",
    "        noise = data['noise']\n",
    "    thermal_noise_pwr = dbinv(noise)\n",
    "    quant_error_pwr = scale * (nrx * ntx)\n",
    "    total_noise_pwr = thermal_noise_pwr + quant_error_pwr\n",
    "    ret = csi * sqrt(scale / total_noise_pwr)\n",
    "    if ntx == 2:\n",
    "        ret = ret * sqrt(2)\n",
    "    elif ntx == 3:\n",
    "        ret = ret * sqrt(dbinv(4.5))\n",
    "    return ret\n",
    "\n",
    "def expandable_or(x, y):\n",
    "    z = x | y\n",
    "    low = z & 0xff\n",
    "    return unpack('b', pack('B', low))[0]\n",
    "\n",
    "def read_bfree(array):\n",
    "    result = {}\n",
    "    timestamp_low = array[0] + (array[1] << 8) + (array[2] << 16) + (array[3] << 24)\n",
    "    bf_count = array[4] + (array[5] << 8)\n",
    "    nrx = array[8]  # 接收天线的数目\n",
    "    ntx = array[9]\n",
    "    rssi_a = array[10]\n",
    "    rssi_b = array[11]\n",
    "    rssi_c = array[12]\n",
    "    # noise\n",
    "    noise = unpack('b', pack('B', array[13]))[0]\n",
    "    agc = array[14]\n",
    "    antenna_sel = array[15]\n",
    "    length = array[16] + (array[17] << 8)\n",
    "    fake_rate_n_flags = array[18] + (array[19] << 8)\n",
    "    calc_len = (30 * (nrx * ntx * 8 * 2 + 3) + 7) // 8\n",
    "    payload = array[20:]  # csi数据部分\n",
    "\n",
    "    if length != calc_len:\n",
    "        print('数据发现错误!')\n",
    "        exit(0)\n",
    "    result['timestamp_low'] = timestamp_low\n",
    "    result['bfree_count'] = bf_count\n",
    "    result['rssi_a'] = rssi_a\n",
    "    result['rssi_b'] = rssi_b\n",
    "    result['rssi_c'] = rssi_c\n",
    "    result['nrx'] = nrx\n",
    "    result['ntx'] = ntx\n",
    "    result['agc'] = agc\n",
    "    result['rate'] = fake_rate_n_flags\n",
    "    result['noise'] = noise\n",
    "    csi = np.zeros((ntx, nrx, 30), dtype=np.complex64)\n",
    "    # 现在开始构建numpy array\n",
    "    idx = 0        \n",
    "    for sub_idx in range(30):\n",
    "        idx = idx + 3\n",
    "        remainder = idx % 8  # 余数\n",
    "        for m in range(nrx):\n",
    "            for n in range(ntx):\n",
    "                real = expandable_or((payload[idx // 8] >> remainder), (payload[idx // 8 + 1] << (8 - remainder)))\n",
    "                img = expandable_or((payload[idx // 8 + 1] >> remainder), (payload[idx // 8 + 2] << (8 - remainder)))\n",
    "                csi[n, m, sub_idx] = complex(real, img)     # 构建一个复数\n",
    "                idx = idx + 16\n",
    "    result['csi'] = csi\n",
    "    perm = np.zeros(3, dtype=np.uint32)\n",
    "    perm[0] = (antenna_sel & 0x3) + 1\n",
    "    perm[1] = ((antenna_sel >> 2) & 0x3) + 1\n",
    "    perm[2] = ((antenna_sel >> 4) & 0x3) + 1\n",
    "    result['perm'] = perm\n",
    "    return result\n",
    "\n",
    "# 从.dat抽取生成CSI字典数组 2*3*30\n",
    "def extract_csi(file_name):\n",
    "    triangle = np.array([1, 3, 6])\n",
    "    csis = []\n",
    "    with open(file_name, 'rb') as f:\n",
    "        buff = f.read()\n",
    "        curr = 0    # 记录当前已经处理到了的位置\n",
    "        length = len(buff)\n",
    "        while curr < (length - 3):\n",
    "            data_len = unpack('>H', buff[curr:curr+2])[0]  # 实际长度\n",
    "            if data_len > (length - curr - 2):  # 防止越界的错误\n",
    "                break\n",
    "            code = unpack('B', buff[curr+2:curr+3])[0]  # 代码\n",
    "            curr = curr + 3\n",
    "            if code == 187:\n",
    "                # 将CSI数据帧解析\n",
    "                csi_dic = read_bfree(buff[curr:])\n",
    "                perm = csi_dic['perm']\n",
    "                nrx = csi_dic['nrx']\n",
    "                csi = csi_dic['csi']\n",
    "                if sum(perm) == triangle[nrx - 1]:  # 下标从0开始\n",
    "                    csi[:, perm - 1, :] = csi[:, [x for x in range(nrx)], :]\n",
    "                # csi = get_scaled_csi(data)\n",
    "                csis.append(csi_dic)\n",
    "            curr = curr + data_len - 1\n",
    "    return csis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y1P6tiPbdeKx"
   },
   "outputs": [],
   "source": [
    "# 提取每个天线对上的CSI\n",
    "def process_data(raw_file_path, save_path):\n",
    "  X={}\n",
    "  for f in tqdm(os.listdir(raw_file_path)):\n",
    "    if f.endswith('.dat'):\n",
    "      file_name = os.path.join(raw_file_path, f)\n",
    "      extracted_data = extract_csi(file_name)\n",
    "      print('processing {} the length of this file is:{}'.format(file_name, len(extracted_data)))\n",
    "      tx, rx, sub = extracted_data[0]['csi'].shape\n",
    "      data_csi = np.zeros((len(extracted_data), tx, rx, sub), dtype=np.complex64)\n",
    "      for i in range(len(extracted_data)):\n",
    "        data_csi[i] = get_scaled_csi(extracted_data[i])\n",
    "      data_csi = np.clip(np.abs(np.squeeze(data_csi)), 1e-8, 1e100)[:,:,:2,:].reshape(-1, 4, 30)   # (205, 2, 2, 30)\n",
    "      data = np.zeros((data_csi.shape[0],4))  #N*4\n",
    "      for ant in range(4):  # 每个天线对上的CSI变化趋势相同,为节约计算这里选择天线对即可\n",
    "          data_csi_ant = data_csi[:, ant, :]  \n",
    "          b, a = signal.butter(5, 4*2/30, 'low')\n",
    "          var_max = 0\n",
    "          s_max = None\n",
    "          for s in range(30):\n",
    "              carrier_data = signal.lfilter(b, a, data_csi_ant[:, s]) # N*1\n",
    "              length = len(carrier_data)\n",
    "              var_temp = np.var(carrier_data[length//5:3*length//5]) \n",
    "              if var_max < var_temp: \n",
    "                  var_max = var_temp\n",
    "                  s_max = carrier_data\n",
    "          data[:, ant] = s_max\n",
    "      scio.savemat(os.path.join(save_path, f.split('.')[0]+'.mat'), {'csi': data})\n",
    "      X[f]=data\n",
    "  print('all raw file processed')\n",
    "  return X\n",
    "  \n",
    "train_raw_file_path = r'../data/raw/train'\n",
    "train_save_path = r'../data/raw/train'\n",
    "test_raw_file_path = r'../data/processed/test'\n",
    "test_save_path = r'../data/processed/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 27/780 [00:00<00:13, 57.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a2-12.dat the length of this file is:255\n",
      "processing ../data/raw/train/csi-s1-e1-a1-18.dat the length of this file is:315\n",
      "processing ../data/raw/train/csi-s1-e1-a3-27.dat the length of this file is:242\n",
      "processing ../data/raw/train/csi-s1-e1-a1-29.dat the length of this file is:287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 34/780 [00:00<00:12, 60.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a5-40.dat the length of this file is:190\n",
      "processing ../data/raw/train/csi-s1-e1-a1-35.dat the length of this file is:297\n",
      "processing ../data/raw/train/csi-s1-e1-a4-32.dat the length of this file is:409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 41/780 [00:00<00:15, 46.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-42.dat the length of this file is:384\n",
      "processing ../data/raw/train/csi-s1-e1-a1-40.dat the length of this file is:347\n",
      "processing ../data/raw/train/csi-s1-e1-a2-5.dat the length of this file is:281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 57/780 [00:00<00:14, 50.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-45.dat the length of this file is:409\n",
      "processing ../data/raw/train/csi-s1-e1-a5-24.dat the length of this file is:183\n",
      "processing ../data/raw/train/csi-s1-e1-a2-29.dat the length of this file is:247\n",
      "processing ../data/raw/train/csi-s1-e1-a3-41.dat the length of this file is:310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 71/780 [00:01<00:13, 52.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a2-15.dat the length of this file is:254\n",
      "processing ../data/raw/train/csi-s1-e1-a1-23.dat the length of this file is:297\n",
      "processing ../data/raw/train/csi-s1-e1-a3-28.dat the length of this file is:284\n",
      "processing ../data/raw/train/csi-s1-e1-a4-35.dat the length of this file is:398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 82/780 [00:01<00:16, 43.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a2-21.dat the length of this file is:304\n",
      "processing ../data/raw/train/csi-s1-e1-a3-43.dat the length of this file is:304\n",
      "processing ../data/raw/train/csi-s1-e1-a2-6.dat the length of this file is:293\n",
      "processing ../data/raw/train/csi-s1-e1-a2-39.dat the length of this file is:314\n",
      "processing ../data/raw/train/csi-s1-e1-a1-33.dat the length of this file is:298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 92/780 [00:01<00:16, 42.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a2-42.dat the length of this file is:256\n",
      "processing ../data/raw/train/csi-s1-e1-a3-49.dat the length of this file is:296\n",
      "processing ../data/raw/train/csi-s1-e1-a1-37.dat the length of this file is:285\n",
      "processing ../data/raw/train/csi-s1-e1-a4-22.dat the length of this file is:354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 97/780 [00:01<00:17, 39.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-16.dat the length of this file is:352\n",
      "processing ../data/raw/train/csi-s1-e1-a4-25.dat the length of this file is:346\n",
      "processing ../data/raw/train/csi-s1-e1-a1-39.dat the length of this file is:302\n",
      "processing ../data/raw/train/csi-s1-e1-a1-36.dat the length of this file is:292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 113/780 [00:02<00:17, 39.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a1-4.dat the length of this file is:360\n",
      "processing ../data/raw/train/csi-s1-e1-a5-38.dat the length of this file is:177\n",
      "processing ../data/raw/train/csi-s1-e1-a5-16.dat the length of this file is:173\n",
      "processing ../data/raw/train/csi-s1-e1-a5-50.dat the length of this file is:177\n",
      "processing ../data/raw/train/csi-s1-e1-a5-45.dat the length of this file is:257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 119/780 [00:02<00:16, 40.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-21.dat the length of this file is:385\n",
      "processing ../data/raw/train/csi-s1-e1-a6-22.dat the length of this file is:168\n",
      "processing ../data/raw/train/csi-s1-e1-a6-29.dat the length of this file is:143\n",
      "processing ../data/raw/train/csi-s1-e1-a6-41.dat the length of this file is:146\n",
      "processing ../data/raw/train/csi-s1-e1-a6-19.dat the length of this file is:179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 134/780 [00:02<00:13, 49.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-24.dat the length of this file is:378\n",
      "processing ../data/raw/train/csi-s1-e1-a1-17.dat the length of this file is:295\n",
      "processing ../data/raw/train/csi-s1-e1-a4-30.dat the length of this file is:405\n",
      "processing ../data/raw/train/csi-s1-e1-a1-49.dat the length of this file is:303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 147/780 [00:02<00:14, 43.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-41.dat the length of this file is:413\n",
      "processing ../data/raw/train/csi-s1-e1-a1-15.dat the length of this file is:294\n",
      "processing ../data/raw/train/csi-s1-e1-a1-7.dat the length of this file is:319\n",
      "processing ../data/raw/train/csi-s1-e1-a4-29.dat the length of this file is:386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 181/780 [00:03<00:11, 53.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a5-18.dat the length of this file is:157\n",
      "processing ../data/raw/train/csi-s1-e1-a1-8.dat the length of this file is:287\n",
      "processing ../data/raw/train/csi-s1-e1-a6-23.dat the length of this file is:125\n",
      "processing ../data/raw/train/csi-s1-e1-a5-34.dat the length of this file is:167\n",
      "processing ../data/raw/train/csi-s1-e1-a2-18.dat the length of this file is:281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▍       | 192/780 [00:03<00:12, 46.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a2-4.dat the length of this file is:274\n",
      "processing ../data/raw/train/csi-s1-e1-a3-26.dat the length of this file is:281\n",
      "processing ../data/raw/train/csi-s1-e1-a6-50.dat the length of this file is:173\n",
      "processing ../data/raw/train/csi-s1-e1-a3-11.dat the length of this file is:235\n",
      "processing ../data/raw/train/csi-s1-e1-a3-25.dat the length of this file is:255\n",
      "processing ../data/raw/train/csi-s1-e1-a6-15.dat the length of this file is:137"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 208/780 [00:03<00:10, 56.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing ../data/raw/train/csi-s1-e1-a1-26.dat the length of this file is:299\n",
      "processing ../data/raw/train/csi-s1-e1-a2-24.dat the length of this file is:241\n",
      "processing ../data/raw/train/csi-s1-e1-a1-42.dat the length of this file is:367\n",
      "processing ../data/raw/train/csi-s1-e1-a4-12.dat the length of this file is:478\n",
      "processing ../data/raw/train/csi-s1-e1-a4-26.dat the length of this file is:364\n",
      "processing ../data/raw/train/csi-s1-e1-a1-48.dat the length of this file is:311\n",
      "processing ../data/raw/train/csi-s1-e1-a1-16.dat the length of this file is:301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 226/780 [00:04<00:12, 44.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-34.dat the length of this file is:412\n",
      "processing ../data/raw/train/csi-s1-e1-a2-16.dat the length of this file is:258\n",
      "processing ../data/raw/train/csi-s1-e1-a3-19.dat the length of this file is:181\n",
      "processing ../data/raw/train/csi-s1-e1-a2-30.dat the length of this file is:264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██▉       | 233/780 [00:04<00:12, 44.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a1-45.dat the length of this file is:298\n",
      "processing ../data/raw/train/csi-s1-e1-a3-23.dat the length of this file is:210\n",
      "processing ../data/raw/train/csi-s1-e1-a1-9.dat the length of this file is:291\n",
      "processing ../data/raw/train/csi-s1-e1-a6-40.dat the length of this file is:136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 248/780 [00:04<00:11, 46.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-48.dat the length of this file is:417\n",
      "processing ../data/raw/train/csi-s1-e1-a3-30.dat the length of this file is:246\n",
      "processing ../data/raw/train/csi-s1-e1-a4-38.dat the length of this file is:416\n",
      "processing ../data/raw/train/csi-s1-e1-a6-45.dat the length of this file is:166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 267/780 [00:04<00:09, 52.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a5-13.dat the length of this file is:162\n",
      "processing ../data/raw/train/csi-s1-e1-a3-34.dat the length of this file is:260\n",
      "processing ../data/raw/train/csi-s1-e1-a2-33.dat the length of this file is:247\n",
      "processing ../data/raw/train/csi-s1-e1-a6-46.dat the length of this file is:166\n",
      "processing ../data/raw/train/csi-s1-e1-a3-36.dat the length of this file is:290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███▌      | 282/780 [00:04<00:08, 61.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a6-20.dat the length of this file is:158\n",
      "processing ../data/raw/train/csi-s1-e1-a1-6.dat the length of this file is:291\n",
      "processing ../data/raw/train/csi-s1-e1-a3-40.dat the length of this file is:267\n",
      "processing ../data/raw/train/csi-s1-e1-a2-20.dat the length of this file is:260\n",
      "processing ../data/raw/train/csi-s1-e1-a6-26.dat the length of this file is:172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 290/780 [00:05<00:08, 56.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a2-27.dat the length of this file is:272\n",
      "processing ../data/raw/train/csi-s1-e1-a6-39.dat the length of this file is:140\n",
      "processing ../data/raw/train/csi-s1-e1-a1-25.dat the length of this file is:299\n",
      "processing ../data/raw/train/csi-s1-e1-a2-9.dat the length of this file is:260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 308/780 [00:05<00:08, 55.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a1-47.dat the length of this file is:291\n",
      "processing ../data/raw/train/csi-s1-e1-a5-17.dat the length of this file is:192\n",
      "processing ../data/raw/train/csi-s1-e1-a2-38.dat the length of this file is:263\n",
      "processing ../data/raw/train/csi-s1-e1-a2-3.dat the length of this file is:282\n",
      "processing ../data/raw/train/csi-s1-e1-a6-24.dat the length of this file is:190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 315/780 [00:05<00:10, 45.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-15.dat the length of this file is:375\n",
      "processing ../data/raw/train/csi-s1-e1-a4-49.dat the length of this file is:494\n",
      "processing ../data/raw/train/csi-s1-e1-a6-12.dat the length of this file is:135\n",
      "processing ../data/raw/train/csi-s1-e1-a3-16.dat the length of this file is:251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 334/780 [00:05<00:08, 54.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a5-21.dat the length of this file is:151\n",
      "processing ../data/raw/train/csi-s1-e1-a5-39.dat the length of this file is:209\n",
      "processing ../data/raw/train/csi-s1-e1-a6-28.dat the length of this file is:156\n",
      "processing ../data/raw/train/csi-s1-e1-a2-10.dat the length of this file is:254\n",
      "processing ../data/raw/train/csi-s1-e1-a6-11.dat the length of this file is:127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▎     | 341/780 [00:06<00:08, 49.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a1-50.dat the length of this file is:304\n",
      "processing ../data/raw/train/csi-s1-e1-a5-49.dat the length of this file is:271\n",
      "processing ../data/raw/train/csi-s1-e1-a1-19.dat the length of this file is:283\n",
      "processing ../data/raw/train/csi-s1-e1-a2-37.dat the length of this file is:275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 356/780 [00:06<00:08, 51.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a1-30.dat the length of this file is:295\n",
      "processing ../data/raw/train/csi-s1-e1-a4-44.dat the length of this file is:402\n",
      "processing ../data/raw/train/csi-s1-e1-a5-37.dat the length of this file is:187\n",
      "processing ../data/raw/train/csi-s1-e1-a6-31.dat the length of this file is:160\n",
      "processing ../data/raw/train/csi-s1-e1-a6-36.dat the length of this file is:174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▋     | 362/780 [00:06<00:09, 44.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a6-16.dat the length of this file is:146\n",
      "processing ../data/raw/train/csi-s1-e1-a3-39.dat the length of this file is:256\n",
      "processing ../data/raw/train/csi-s1-e1-a1-41.dat the length of this file is:306\n",
      "processing ../data/raw/train/csi-s1-e1-a5-36.dat the length of this file is:181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 370/780 [00:06<00:08, 46.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a1-14.dat the length of this file is:300\n",
      "processing ../data/raw/train/csi-s1-e1-a2-31.dat the length of this file is:272\n",
      "processing ../data/raw/train/csi-s1-e1-a6-27.dat the length of this file is:175\n",
      "processing ../data/raw/train/csi-s1-e1-a1-38.dat the length of this file is:287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 381/780 [00:06<00:09, 44.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a2-32.dat the length of this file is:291\n",
      "processing ../data/raw/train/csi-s1-e1-a2-50.dat the length of this file is:307\n",
      "processing ../data/raw/train/csi-s1-e1-a2-36.dat the length of this file is:274\n",
      "processing ../data/raw/train/csi-s1-e1-a2-11.dat the length of this file is:257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 394/780 [00:07<00:08, 46.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a3-21.dat the length of this file is:275\n",
      "processing ../data/raw/train/csi-s1-e1-a5-32.dat the length of this file is:181\n",
      "processing ../data/raw/train/csi-s1-e1-a1-12.dat the length of this file is:362\n",
      "processing ../data/raw/train/csi-s1-e1-a2-40.dat the length of this file is:249\n",
      "processing ../data/raw/train/csi-s1-e1-a5-48.dat the length of this file is:201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 410/780 [00:07<00:06, 55.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a3-38.dat the length of this file is:244\n",
      "processing ../data/raw/train/csi-s1-e1-a2-34.dat the length of this file is:328\n",
      "processing ../data/raw/train/csi-s1-e1-a5-15.dat the length of this file is:171\n",
      "processing ../data/raw/train/csi-s1-e1-a6-37.dat the length of this file is:134\n",
      "processing ../data/raw/train/csi-s1-e1-a3-46.dat the length of this file is:270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 424/780 [00:07<00:06, 51.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a6-48.dat the length of this file is:181\n",
      "processing ../data/raw/train/csi-s1-e1-a1-34.dat the length of this file is:299\n",
      "processing ../data/raw/train/csi-s1-e1-a5-23.dat the length of this file is:179\n",
      "processing ../data/raw/train/csi-s1-e1-a5-41.dat the length of this file is:198\n",
      "processing ../data/raw/train/csi-s1-e1-a5-22.dat the length of this file is:178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 430/780 [00:07<00:06, 53.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a1-13.dat the length of this file is:309\n",
      "processing ../data/raw/train/csi-s1-e1-a1-3.dat the length of this file is:291\n",
      "processing ../data/raw/train/csi-s1-e1-a4-43.dat the length of this file is:386\n",
      "processing ../data/raw/train/csi-s1-e1-a5-19.dat the length of this file is:176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 446/780 [00:08<00:06, 49.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a5-43.dat the length of this file is:172\n",
      "processing ../data/raw/train/csi-s1-e1-a6-44.dat the length of this file is:165\n",
      "processing ../data/raw/train/csi-s1-e1-a6-49.dat the length of this file is:177\n",
      "processing ../data/raw/train/csi-s1-e1-a3-35.dat the length of this file is:293\n",
      "processing ../data/raw/train/csi-s1-e1-a3-12.dat the length of this file is:208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▊    | 457/780 [00:08<00:05, 55.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a3-18.dat the length of this file is:245\n",
      "processing ../data/raw/train/csi-s1-e1-a2-17.dat the length of this file is:246\n",
      "processing ../data/raw/train/csi-s1-e1-a4-33.dat the length of this file is:401\n",
      "processing ../data/raw/train/csi-s1-e1-a3-13.dat the length of this file is:250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 471/780 [00:08<00:05, 52.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a3-50.dat the length of this file is:253\n",
      "processing ../data/raw/train/csi-s1-e1-a2-48.dat the length of this file is:263\n",
      "processing ../data/raw/train/csi-s1-e1-a6-33.dat the length of this file is:121\n",
      "processing ../data/raw/train/csi-s1-e1-a5-30.dat the length of this file is:163\n",
      "processing ../data/raw/train/csi-s1-e1-a2-45.dat the length of this file is:258\n",
      "processing ../data/raw/train/csi-s1-e1-a6-35.dat the length of this file is:166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 486/780 [00:08<00:05, 54.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a1-28.dat the length of this file is:297\n",
      "processing ../data/raw/train/csi-s1-e1-a5-44.dat the length of this file is:187\n",
      "processing ../data/raw/train/csi-s1-e1-a5-29.dat the length of this file is:170\n",
      "processing ../data/raw/train/csi-s1-e1-a2-7.dat the length of this file is:244\n",
      "processing ../data/raw/train/csi-s1-e1-a4-23.dat the length of this file is:418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 500/780 [00:09<00:04, 57.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a5-27.dat the length of this file is:156\n",
      "processing ../data/raw/train/csi-s1-e1-a2-47.dat the length of this file is:300\n",
      "processing ../data/raw/train/csi-s1-e1-a3-48.dat the length of this file is:273\n",
      "processing ../data/raw/train/csi-s1-e1-a5-31.dat the length of this file is:164\n",
      "processing ../data/raw/train/csi-s1-e1-a3-42.dat the length of this file is:274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|██████▌   | 514/780 [00:09<00:03, 68.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a6-14.dat the length of this file is:117\n",
      "processing ../data/raw/train/csi-s1-e1-a3-32.dat the length of this file is:263\n",
      "processing ../data/raw/train/csi-s1-e1-a1-31.dat the length of this file is:274\n",
      "processing ../data/raw/train/csi-s1-e1-a2-28.dat the length of this file is:255\n",
      "processing ../data/raw/train/csi-s1-e1-a6-47.dat the length of this file is:150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 522/780 [00:09<00:05, 51.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a1-5.dat the length of this file is:297\n",
      "processing ../data/raw/train/csi-s1-e1-a1-1.dat the length of this file is:312\n",
      "processing ../data/raw/train/csi-s1-e1-a4-47.dat the length of this file is:407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 535/780 [00:09<00:05, 42.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-13.dat the length of this file is:382\n",
      "processing ../data/raw/train/csi-s1-e1-a2-2.dat the length of this file is:250\n",
      "processing ../data/raw/train/csi-s1-e1-a4-14.dat the length of this file is:377\n",
      "processing ../data/raw/train/csi-s1-e1-a6-21.dat the length of this file is:144\n",
      "processing ../data/raw/train/csi-s1-e1-a3-24.dat the length of this file is:257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 563/780 [00:10<00:03, 56.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a5-20.dat the length of this file is:164\n",
      "processing ../data/raw/train/csi-s1-e1-a2-25.dat the length of this file is:268\n",
      "processing ../data/raw/train/csi-s1-e1-a3-17.dat the length of this file is:235\n",
      "processing ../data/raw/train/csi-s1-e1-a4-11.dat the length of this file is:404\n",
      "processing ../data/raw/train/csi-s1-e1-a3-45.dat the length of this file is:294\n",
      "processing ../data/raw/train/csi-s1-e1-a6-38.dat the length of this file is:131\n",
      "processing ../data/raw/train/csi-s1-e1-a4-19.dat the length of this file is:415\n",
      "processing ../data/raw/train/csi-s1-e1-a2-44.dat the length of this file is:269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 578/780 [00:10<00:04, 43.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a2-19.dat the length of this file is:267\n",
      "processing ../data/raw/train/csi-s1-e1-a5-47.dat the length of this file is:223\n",
      "processing ../data/raw/train/csi-s1-e1-a3-31.dat the length of this file is:254\n",
      "processing ../data/raw/train/csi-s1-e1-a4-17.dat the length of this file is:383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 594/780 [00:10<00:03, 53.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a2-41.dat the length of this file is:247\n",
      "processing ../data/raw/train/csi-s1-e1-a5-26.dat the length of this file is:193\n",
      "processing ../data/raw/train/csi-s1-e1-a2-8.dat the length of this file is:256\n",
      "processing ../data/raw/train/csi-s1-e1-a2-14.dat the length of this file is:264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 621/780 [00:10<00:02, 68.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-40.dat the length of this file is:482\n",
      "processing ../data/raw/train/csi-s1-e1-a5-35.dat the length of this file is:196\n",
      "processing ../data/raw/train/csi-s1-e1-a6-30.dat the length of this file is:166\n",
      "processing ../data/raw/train/csi-s1-e1-a1-22.dat the length of this file is:313\n",
      "processing ../data/raw/train/csi-s1-e1-a6-43.dat the length of this file is:149\n",
      "processing ../data/raw/train/csi-s1-e1-a6-17.dat the length of this file is:146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 646/780 [00:11<00:01, 80.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-36.dat the length of this file is:393\n",
      "processing ../data/raw/train/csi-s1-e1-a1-46.dat the length of this file is:296\n",
      "processing ../data/raw/train/csi-s1-e1-a3-47.dat the length of this file is:263\n",
      "processing ../data/raw/train/csi-s1-e1-a5-14.dat the length of this file is:197\n",
      "processing ../data/raw/train/csi-s1-e1-a5-46.dat the length of this file is:240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████▍ | 656/780 [00:11<00:01, 65.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a3-20.dat the length of this file is:234\n",
      "processing ../data/raw/train/csi-s1-e1-a3-15.dat the length of this file is:237\n",
      "processing ../data/raw/train/csi-s1-e1-a5-42.dat the length of this file is:199\n",
      "processing ../data/raw/train/csi-s1-e1-a6-25.dat the length of this file is:166\n",
      "processing ../data/raw/train/csi-s1-e1-a1-32.dat the length of this file is:290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 666/780 [00:11<00:01, 67.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a3-14.dat the length of this file is:233\n",
      "processing ../data/raw/train/csi-s1-e1-a5-25.dat the length of this file is:168\n",
      "processing ../data/raw/train/csi-s1-e1-a4-27.dat the length of this file is:412\n",
      "processing ../data/raw/train/csi-s1-e1-a1-43.dat the length of this file is:287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 692/780 [00:11<00:01, 61.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-28.dat the length of this file is:426\n",
      "processing ../data/raw/train/csi-s1-e1-a5-33.dat the length of this file is:176\n",
      "processing ../data/raw/train/csi-s1-e1-a3-33.dat the length of this file is:240\n",
      "processing ../data/raw/train/csi-s1-e1-a3-44.dat the length of this file is:323\n",
      "processing ../data/raw/train/csi-s1-e1-a6-42.dat the length of this file is:139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████▉ | 701/780 [00:12<00:01, 53.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a1-11.dat the length of this file is:280\n",
      "processing ../data/raw/train/csi-s1-e1-a2-46.dat the length of this file is:270\n",
      "processing ../data/raw/train/csi-s1-e1-a4-37.dat the length of this file is:428\n",
      "processing ../data/raw/train/csi-s1-e1-a5-11.dat the length of this file is:218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 709/780 [00:12<00:01, 52.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a1-10.dat the length of this file is:296\n",
      "processing ../data/raw/train/csi-s1-e1-a1-27.dat the length of this file is:294\n",
      "processing ../data/raw/train/csi-s1-e1-a3-37.dat the length of this file is:257\n",
      "processing ../data/raw/train/csi-s1-e1-a6-18.dat the length of this file is:158\n",
      "processing ../data/raw/train/csi-s1-e1-a2-49.dat the length of this file is:290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 729/780 [00:12<00:01, 49.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a5-28.dat the length of this file is:211\n",
      "processing ../data/raw/train/csi-s1-e1-a2-1.dat the length of this file is:252\n",
      "processing ../data/raw/train/csi-s1-e1-a4-50.dat the length of this file is:411\n",
      "processing ../data/raw/train/csi-s1-e1-a2-13.dat the length of this file is:257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 736/780 [00:12<00:00, 44.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-39.dat the length of this file is:377\n",
      "processing ../data/raw/train/csi-s1-e1-a1-44.dat the length of this file is:289\n",
      "processing ../data/raw/train/csi-s1-e1-a1-24.dat the length of this file is:308\n",
      "processing ../data/raw/train/csi-s1-e1-a2-22.dat the length of this file is:241\n",
      "processing ../data/raw/train/csi-s1-e1-a6-13.dat the length of this file is:150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 747/780 [00:13<00:00, 43.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a6-32.dat the length of this file is:104\n",
      "processing ../data/raw/train/csi-s1-e1-a2-23.dat the length of this file is:248\n",
      "processing ../data/raw/train/csi-s1-e1-a1-21.dat the length of this file is:299\n",
      "processing ../data/raw/train/csi-s1-e1-a2-35.dat the length of this file is:302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 758/780 [00:13<00:00, 44.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a2-43.dat the length of this file is:257\n",
      "processing ../data/raw/train/csi-s1-e1-a1-20.dat the length of this file is:293\n",
      "processing ../data/raw/train/csi-s1-e1-a4-18.dat the length of this file is:394\n",
      "processing ../data/raw/train/csi-s1-e1-a3-29.dat the length of this file is:274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 763/780 [00:13<00:00, 39.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a4-46.dat the length of this file is:448\n",
      "processing ../data/raw/train/csi-s1-e1-a4-31.dat the length of this file is:372\n",
      "processing ../data/raw/train/csi-s1-e1-a4-20.dat the length of this file is:369\n",
      "processing ../data/raw/train/csi-s1-e1-a6-34.dat the length of this file is:178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [00:13<00:00, 56.43it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../data/raw/train/csi-s1-e1-a5-12.dat the length of this file is:219\n",
      "processing ../data/raw/train/csi-s1-e1-a3-22.dat the length of this file is:261\n",
      "processing ../data/raw/train/csi-s1-e1-a1-2.dat the length of this file is:353\n",
      "processing ../data/raw/train/csi-s1-e1-a2-26.dat the length of this file is:292\n",
      "all raw file processed\n",
      "all raw file processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = process_data(train_raw_file_path, train_save_path)\n",
    "X_test = process_data(test_raw_file_path, test_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aOHTXJJpZ6hm",
    "outputId": "5cb39d18-70ba-4140-9cac-e4de2bc081e5"
   },
   "outputs": [],
   "source": [
    "# src_path=r'/content/drive/My Drive/SR_CSI/Gestures_data/train'\n",
    "# target_path=r'/content/drive/My Drive/SR_CSI/Gestures_data/test'\n",
    "# a=[]\n",
    "# for i in os.listdir(target_path):\n",
    "#   a.append(i)\n",
    "# print(len(a))\n",
    "#   if i.endswith('.mat'):\n",
    "#     os.remove(os.path.join(src_path,i))\n",
    "# import os,shutil\n",
    "# for i in range(1,7):\n",
    "#   for j in range(6, 11):\n",
    "#     shutil.move(src_path+r'/csi-s1-e1-a{}-{}.txt'.format(i,j),target_path+r'/csi-s1-e1-a{}-{}.txt'.format(i,j))   \n",
    "# # X_train = process_data(train_raw_file_path, train_save_path)\n",
    "# # test_raw_file_path = r'/content/drive/My Drive/SR_CSI/Gestures_data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "StvpPxJOnz8T"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 780/780 [00:00<00:00, 14165.96it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 重新构造mat文件选择最佳子载波上的CSI\n",
    "best_index_list = [1, 3, 3, 3, 2, 3]\n",
    "#\n",
    "for f in tqdm(os.listdir(train_save_path)):\n",
    "    if f.endswith('.mat'):\n",
    "      file_name = os.path.join(train_save_path, f)\n",
    "      activate_index = int(re.findall(re.compile(r'csi-s1-e1-a(.*?)-.*?.mat', re.S), f)[0])\n",
    "      csi = np.squeeze(scio.loadmat(file_name)['csi'])\n",
    "      best_csi = csi[:, best_index_list[activate_index-1]]     \n",
    "      scio.savemat(file_name, {'csi': best_csi})\n",
    "for f in tqdm(os.listdir(test_save_path)):\n",
    "    if f.endswith('.mat'):\n",
    "      file_name = os.path.join(test_save_path, f)\n",
    "      activate_index = int(re.findall(re.compile(r'csi-s1-e1-a(.*?)-.*?.mat', re.S), f)[0])\n",
    "      best_csi = np.squeeze(scio.loadmat(file_name)['csi'])[:, best_index_list[activate_index-1]]\n",
    "      scio.savemat(file_name, {'csi': best_csi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXNfstcbnNPG"
   },
   "outputs": [],
   "source": [
    "# --抖动,添加噪声超参数：sigma =噪声的标准偏差（STD）\n",
    "def add_jitter(X, sigma=0.1):  \n",
    "    myNoise = np.random.normal(loc=0, scale=sigma, size=X.shape)\n",
    "    # plt.plot(myNoise, label='noise')\n",
    "    return X+myNoise\n",
    "\n",
    "# --缩放¶超参数:σ=放大/缩小系数的标准值通过乘以一个随机标量来更改窗口中数据的大小\n",
    "def add_scaling(X, sigma=0.2):\n",
    "    scalingFactor = np.random.normal(loc=1.0, scale=sigma, size=(1))\n",
    "    myNoise = np.matmul(np.ones((X.shape[0],1)), scalingFactor)\n",
    "    return X*myNoise\n",
    "\n",
    "# 降采样,使用一组降采样因子 k1, k2, k3，每隔 ki-1 个数据取一个。\n",
    "def down_sampling(data, rate=1):\n",
    "    # down sampling by rate k\n",
    "    if rate > data.shape[0] / 3:\n",
    "        print('sampling rate is too high')\n",
    "        return None\n",
    "    ds_data = data[::rate]  # temp after down sampling\n",
    "    ds_data_len = ds_data.shape[0]  # remark the length info\n",
    "    return ds_data \n",
    "\n",
    "# --滑动平均 使用一组滑动窗口l1, l2, l3，每li个数据取平均\n",
    "def moving_average(data, moving_wl=10):\n",
    "    data_len = data.shape[0]\n",
    "    if  moving_wl > data.shape[0] / 3:\n",
    "        print('moving window is too high')\n",
    "        return None\n",
    "    ma_data = np.zeros(data_len-moving_wl+1)\n",
    "    for i in range(data_len-moving_wl+1):\n",
    "        ma_data[i] = np.mean(data[i: i+moving_wl])\n",
    "    return ma_data\n",
    "\n",
    "# ------------------裁剪（Crop） 使用滑动窗口在时间序列上截取数据\n",
    "def data_crop(data, wl_ratio=0.8):\n",
    "    data_len = data.shape[0]\n",
    "    wl = int(data_len*wl_ratio)\n",
    "    start = int(data_len*(1-wl_ratio)//2)\n",
    "    end = start + wl\n",
    "    #print(start, end)\n",
    "    crop_data = data[start:end]\n",
    "    return crop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QVcD3THEupbw"
   },
   "outputs": [],
   "source": [
    "#　加载数据生成特征，供网络训练\n",
    "# 读取文件,生成音频文件和标签文件列表\n",
    "def get_file_list(train_file):\n",
    "    label_f_list = []\n",
    "    wav_f_list = []\n",
    "    for root, dirs, files in os.walk(train_file):\n",
    "        for file in files:\n",
    "            if file.endswith('.mat') or file.endswith('.MAT'):\n",
    "                wav_file = os.sep.join([root, file])\n",
    "                label_file = wav_file.split('.mat')[0] + '.txt'\n",
    "                wav_f_list.append(wav_file)\n",
    "                label_f_list.append(label_file)\n",
    "    return label_f_list, wav_f_list\n",
    "\n",
    "# label数据处理\n",
    "def get_label_data(label_f_list):\n",
    "    # 生成label_data每个文件里的声音标签集合\n",
    "    l_list = []\n",
    "    for label_file in tqdm(label_f_list):\n",
    "        with open(label_file, 'r', encoding='utf8') as ff:\n",
    "            try:\n",
    "                data = ff.read()\n",
    "            except:\n",
    "                print(label_file, 'not get label data')\n",
    "            l_list.append(data)\n",
    "    return l_list\n",
    "\n",
    "# 为label建立词典\n",
    "def gen_py_list(label_batch):\n",
    "    p_list = []\n",
    "    for li in label_batch:\n",
    "        l = li.split('  ')\n",
    "        for pny in l:\n",
    "            if pny not in p_list:\n",
    "                p_list.append(pny)\n",
    "    p_list.append('_')  # 该帧可能是空\n",
    "    return p_list\n",
    "\n",
    "# 将读取到的label映射到对应的id\n",
    "def py2id(l, p_list):\n",
    "    ids = []\n",
    "    for py in l.split('  '):\n",
    "        try:\n",
    "          ids.append(p_list.index(py))\n",
    "        except ValueError:\n",
    "          print(py, 'is not in py_llist')\n",
    "    return ids\n",
    "\n",
    "# 对label进行padding和长度获取，不同的是数据维度不同，且label的长度就是输入给ctc的长度，不需要额外处理\n",
    "def label_padding(label_batch):\n",
    "    label_lens = np.array([len(l) for l in label_batch])\n",
    "    label_max_len = max(label_lens)\n",
    "    new_label_batch = np.zeros((len(label_batch), label_max_len))\n",
    "    for j in range(len(label_batch)):\n",
    "        new_label_batch[j][:len(label_batch[j])] = label_batch[j]\n",
    "    return new_label_batch, label_lens\n",
    "\n",
    "# 统一batch内数据：[batch_size, time_step, feature_dim],除此之外，ctc需要获得的信息还有输入序列的长度。\n",
    "def wav_padding(image_size, wav_batch):\n",
    "    wav_lens = [len(w) for w in wav_batch]\n",
    "    wav_max_len = max(wav_lens)\n",
    "    # 每一个sample的时间长都不一样，选择batch内最长的那个时间为基准，进行padding。\n",
    "    new_wav_batch = np.zeros((len(wav_batch), wav_max_len, image_size,1))#wav_max_len, len(wav_batch[0][0], 1))) \n",
    "    # 需要构成成一个tensorflow块，这就要求每个样本数据形式是一样的。\n",
    "    for j in range(len(wav_batch)):\n",
    "        new_wav_batch[j, :wav_batch[j].shape[0], :, 0] = wav_batch[j]\n",
    "    # !!!!!!!3个maxpooling层数据的每个维度需要能够被8整除。因此我们训练实际输入的数据为wav_len//8。!!!!!!!!!!!!\n",
    "    wav_length = np.array([j // 8 for j in wav_lens])\n",
    "    return new_wav_batch, wav_length\n",
    "\n",
    "# 生成batch_size的信号时频图和标签数据，存放到两个list中去\n",
    "def get_batch_generator(b_size, w_list, l_list, p_list, image_size):\n",
    "    shuffle_list = [i for i in range(len(w_list))]\n",
    "    while True:\n",
    "        for j in range(len(w_list)//b_size):\n",
    "            shuffle(shuffle_list)  # 打乱数据的顺序，我们通过查询乱序的索引值，来确定训练数据的顺序\n",
    "            wav_batch = []\n",
    "            label_batch = []\n",
    "            begin = j*b_size\n",
    "            end = begin+b_size\n",
    "            for index in shuffle_list[begin:end]:\n",
    "                fbank = compute_fbank_filt(w_list[index], image_size)\n",
    "                # !!!!!!!3个maxpooling层数据的每个维度需要能够被8整除。因此我们训练实际输入的数据为wav_len//8。!!!!!!!!!!!!             \n",
    "                pad_fbank = np.zeros((image_size//8*8+8,image_size))#fbank.shape[0]//8*8+8, fbank.shape[1], 3))\n",
    "                pad_fbank[:fbank.shape[0], :] = fbank\n",
    "                label = py2id(l_list[index], p_list)\n",
    "                wav_batch.append(pad_fbank)\n",
    "                label_batch.append(label)\n",
    "            pad_wav_data, wav_length = wav_padding(image_size, wav_batch)\n",
    "            pad_label_data, label_length = label_padding(label_batch)\n",
    "            input_batch = {'the_inputs': pad_wav_data,\n",
    "                    'the_labels': pad_label_data,\n",
    "                    'input_length': wav_length,\n",
    "                    'label_length': label_length}\n",
    "            output_batch = {'ctc': np.zeros(pad_wav_data.shape[0])}\n",
    "\n",
    "            yield input_batch, output_batch\n",
    "def get_train_data(w_list, l_list, p_list, image_size):\n",
    "    wav_batch, label_batch = [], []\n",
    "    for index in range(len(w_list)):\n",
    "        X = np.squeeze(scio.loadmat(w_list[index])['csi'])\n",
    "        #print(type(X),X.shape)\n",
    "        gasf = GADF(image_size)\n",
    "        X_gasf = gasf.fit_transform(X.reshape(1, -1))\n",
    "        fbank = X_gasf[0]\n",
    "        # !!!!!!!3个maxpooling层数据的每个维度需要能够被8整除。因此我们训练实际输入的数据为wav_len//8。!!!!!!!!!!!!             \n",
    "        pad_fbank = np.zeros((image_size//8*8+8,image_size))#fbank.shape[0]//8*8+8, fbank.shape[1], 3))\n",
    "        pad_fbank[:fbank.shape[0], :] = fbank\n",
    "        wav_batch.append(pad_fbank)\n",
    "        label = py2id(l_list[index], p_list)\n",
    "        label_batch.append(label)\n",
    "\n",
    "        X1 = add_jitter(X)\n",
    "        X_gasf1 = gasf.fit_transform(X1.reshape(1, -1))\n",
    "        fbank1 = X_gasf1[0]\n",
    "        # !!!!!!!3个maxpooling层数据的每个维度需要能够被8整除。因此我们训练实际输入的数据为wav_len//8。!!!!!!!!!!!!             \n",
    "        pad_fbank1 = np.zeros((image_size//8*8+8,image_size))#fbank.shape[0]//8*8+8, fbank.shape[1], 3))\n",
    "        pad_fbank1[:fbank.shape[0], :] = fbank1\n",
    "        wav_batch.append(pad_fbank1)\n",
    "        label_batch.append(label)\n",
    "\n",
    "        X2 = add_scaling(X)\n",
    "        X_gasf2 = gasf.fit_transform(X2.reshape(1, -1))\n",
    "        fbank2 = X_gasf2[0]\n",
    "        # !!!!!!!3个maxpooling层数据的每个维度需要能够被8整除。因此我们训练实际输入的数据为wav_len//8。!!!!!!!!!!!!             \n",
    "        pad_fbank2 = np.zeros((image_size//8*8+8,image_size))#fbank.shape[0]//8*8+8, fbank.shape[1], 3))\n",
    "        pad_fbank2[:fbank.shape[0], :] = fbank2\n",
    "        wav_batch.append(pad_fbank2)\n",
    "        label_batch.append(label)\n",
    "\n",
    "        X3 = down_sampling(X)\n",
    "        X_gasf3 = gasf.fit_transform(X3.reshape(1, -1))\n",
    "        fbank3 = X_gasf3[0]\n",
    "        # !!!!!!!3个maxpooling层数据的每个维度需要能够被8整除。因此我们训练实际输入的数据为wav_len//8。!!!!!!!!!!!!             \n",
    "        pad_fbank3 = np.zeros((image_size//8*8+8,image_size))#fbank.shape[0]//8*8+8, fbank.shape[1], 3))\n",
    "        pad_fbank3[:fbank.shape[0], :] = fbank3\n",
    "        wav_batch.append(pad_fbank3)\n",
    "        label_batch.append(label)\n",
    "\n",
    "        X4 = moving_average(X)\n",
    "        X_gasf4 = gasf.fit_transform(X4.reshape(1, -1))\n",
    "        fbank4 = X_gasf4[0]\n",
    "        # !!!!!!!3个maxpooling层数据的每个维度需要能够被8整除。因此我们训练实际输入的数据为wav_len//8。!!!!!!!!!!!!             \n",
    "        pad_fbank4 = np.zeros((image_size//8*8+8,image_size))#fbank.shape[0]//8*8+8, fbank.shape[1], 3))\n",
    "        pad_fbank4[:fbank.shape[0], :] = fbank4\n",
    "        wav_batch.append(pad_fbank4)\n",
    "        label_batch.append(label)\n",
    "\n",
    "        X5 = data_crop(X)\n",
    "        X_gasf5 = gasf.fit_transform(X5.reshape(1, -1))\n",
    "        fbank5 = X_gasf5[0]\n",
    "        # !!!!!!!3个maxpooling层数据的每个维度需要能够被8整除。因此我们训练实际输入的数据为wav_len//8。!!!!!!!!!!!!             \n",
    "        pad_fbank5 = np.zeros((image_size//8*8+8,image_size))#fbank.shape[0]//8*8+8, fbank.shape[1], 3))\n",
    "        pad_fbank5[:fbank.shape[0], :] = fbank5\n",
    "        wav_batch.append(pad_fbank5)\n",
    "        label_batch.append(label)\n",
    "          \n",
    "    pad_wav_data, wav_length = wav_padding(image_size, wav_batch)\n",
    "    pad_label_data, label_length = label_padding(label_batch)\n",
    "    inputs = {'the_inputs': pad_wav_data,\n",
    "          'the_labels': pad_label_data,\n",
    "          'input_length': wav_length,\n",
    "          'label_length': label_length}\n",
    "    outputs = {'ctc': np.zeros(pad_wav_data.shape[0])}\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    " # 生成的信号时频图和标签数据，存放到两个list中去\n",
    "def get_test_data(image_size, w_list, l_list, p_list):\n",
    "    input_data = []\n",
    "    output_data = []\n",
    "    for i in range(len(w_list)):\n",
    "        wav_batch = []\n",
    "        label_batch = []\n",
    "\n",
    "        X = np.squeeze(scio.loadmat(w_list[i])['csi'])\n",
    "        #print(type(X),X.shape)\n",
    "        gasf = GADF(image_size)\n",
    "        X_gasf = gasf.fit_transform(X.reshape(1, -1))\n",
    "        fbank = X_gasf[0]\n",
    "        # !!!!!!!3个maxpooling层数据的每个维度需要能够被8整除。因此我们训练实际输入的数据为wav_len//8。!!!!!!!!!!!!             \n",
    "        pad_fbank = np.zeros((image_size//8*8+8,image_size))#fbank.shape[0]//8*8+8, fbank.shape[1], 3))\n",
    "        pad_fbank[:fbank.shape[0], :] = fbank\n",
    "        wav_batch.append(pad_fbank)\n",
    "        label = py2id(l_list[i], p_list)\n",
    "        label_batch.append(label[0])\n",
    "        pad_wav_data, wav_length = wav_padding(image_size, wav_batch)\n",
    "        input_data.append(pad_wav_data)                                        \n",
    "        output_data.append(label[0])\n",
    "    return input_data, output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kTuKvE_K24N6"
   },
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MyUsHoYK25Nd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:00<00:00, 114274.24it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py_list: ['6', '1', '5', '2', '3', '4', '_']\n",
      "train files amount: 260 260 label amount: 7\n",
      "test files amount: 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_file_path = train_save_path\n",
    "train_label_file_list, train_wav_file_list = get_file_list(train_file_path)\n",
    "\n",
    "train_label_list = get_label_data(train_label_file_list)\n",
    "\n",
    "# 用迭代器的时候需要\n",
    "# train_wav_file_list, validate_wav_file_list, train_label_list, validate_label_list = train_test_split(train_wav_file_list, train_label_list, test_size=0.2, random_state=0)\n",
    "\n",
    "# 每个文件拼音标签的集合\n",
    "with open('label_list.txt', 'w') as f:  \n",
    "    f.write('\\n'.join(train_label_list))  \n",
    "py_list = gen_py_list(train_label_list)  # 拼音的集合\n",
    "\n",
    "with open('pinyin_list.txt', 'w') as f:\n",
    "    f.write('\\n'.join(py_list))  # 保存拼音列表\n",
    "\n",
    "train_file_nums = len(train_wav_file_list)\n",
    "# validate_file_nums = len(validate_wav_file_list)\n",
    "\n",
    "py_list_size = len(py_list)  # 模型输出的维度\n",
    "print('py_list:', py_list)\n",
    "print('train files amount:', len(train_label_list), train_file_nums, 'label amount:', py_list_size)#, 'validate files amount:', len(validate_label_list), validate_file_nums,)\n",
    "\n",
    "# 测试数据\n",
    "test_file_path = test_save_path # r'/content/drive/My Drive/data_thchs30'\n",
    "test_label_file_list, test_wav_file_list = get_file_list(test_file_path)\n",
    "\n",
    "test_label_list = get_label_data(test_label_file_list)\n",
    "print('test files amount:',len(test_label_file_list), len(test_wav_file_list))\n",
    "test_data_num = len(test_label_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kK6ZE6xCe6K6"
   },
   "source": [
    "# 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_x6ikrne6K8"
   },
   "outputs": [],
   "source": [
    "# 添加CTC损失函数\n",
    "def ctc_lambda(args):\n",
    "  labels, y_pred, input_length, label_length = args\n",
    "  y_pred = y_pred[:, :, :]\n",
    "  return tf.keras.backend.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "# 定义解码器\n",
    "def decode_ctc(preds, py_list):\n",
    "    window_num = np.zeros((1), dtype=np.int32)\n",
    "    window_num[0] = preds.shape[1]\n",
    "    decode = keras.backend.ctc_decode(preds, window_num, greedy=True, beam_width=100, top_paths=1)\n",
    "    result_index = keras.backend.get_value(decode[0][0])[0]\n",
    "    result_py = []\n",
    "    for i in result_index:\n",
    "        try:\n",
    "            result_py.append(py_list[i])\n",
    "        except IndexError:\n",
    "            print(i, 'not in py_list')\n",
    "    return result_index, result_py\n",
    "\n",
    "def create_model(input_size, output_size):\n",
    "    inputs = Input(name='the_inputs', shape=input_size)\n",
    "    # 1\n",
    "    h1_1 = Conv2D(64, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_1-1')(inputs)\n",
    "    h1_2 = BatchNormalization(name='BatchNormal_1-1')(h1_1)\n",
    "    h1_3 = Conv2D(64, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_1-2')(h1_2)\n",
    "    h1_4 = BatchNormalization(name='BatchNormal_1-2')(h1_3)\n",
    "    h1_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_1')(h1_4)\n",
    "    # 2\n",
    "    h2_1 = Conv2D(128, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_2-1')(h1_5)\n",
    "    h2_2 = BatchNormalization(name='BatchNormal_2-1')(h2_1)\n",
    "    h2_3 = Conv2D(128, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_2-2')(h2_2)\n",
    "    h2_4 = BatchNormalization(name='BatchNormal_2-2')(h2_3)\n",
    "    h2_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_2')(h2_4)\n",
    "    # 3\n",
    "    h3_1 = Conv2D(256, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_3-1')(h2_5)\n",
    "    h3_2 = BatchNormalization(name='BatchNormal_3-1')(h3_1)\n",
    "    h3_3 = Conv2D(256, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_3-2')(h3_2)\n",
    "    h3_4 = BatchNormalization(name='BatchNormal_3-2')(h3_3)\n",
    "    h3_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_3')(h3_4)\n",
    "    # 4\n",
    "    h4_1 = Conv2D(512, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_4-1')(h3_5)\n",
    "    h4_2 = BatchNormalization(name='BatchNormal_4-1')(h4_1)\n",
    "    h4_3 = Conv2D(512, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_4-2')(h4_2)\n",
    "    h4_4 = BatchNormalization(name='BatchNormal_4-2')(h4_3)\n",
    "    # 由于声学模型网络结构原因（3个maxpooling层），我们的音频数据的每个维度需要能够被8整除。这里输入序列经过卷积网络后，长度缩短了8倍，因此我们训练实际输入的数据为wav_len//8。\n",
    "    h5_1 = Reshape((-1, int(input_size[1]//8*512)), name='Reshape_1')(h4_4)\n",
    "    lstm_1 = LSTM(128, return_sequences=True, kernel_initializer='he_normal', name='lstm1')(h5_1)\n",
    "    lstm_2 = LSTM(256, return_sequences=True, kernel_initializer='he_normal', name='lstm2')(lstm_1)\n",
    "    \n",
    "    h5_2 = Dense(512, activation='relu', use_bias=True, kernel_initializer='he_normal', name='Dense_1')(lstm_2)\n",
    "    h5_3 = Dense(output_size, activation=\"relu\", use_bias=True, kernel_initializer='he_normal', name='Dense_2')(h5_2)#(layer_h15)\n",
    "\n",
    "    outputs = Activation('softmax', name='Activation_1')(h5_3)\n",
    "\n",
    "    base_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    labels = Input(name='the_labels', shape=[None], dtype='float32')\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "    # keras.Lambda(function, output_shape=None, mask=None, arguments=None)\n",
    "    # 将任意表达式封装为 Layer 对象\n",
    "    loss_out = Lambda(ctc_lambda, output_shape=(1,), name='ctc')([labels, outputs, input_length, label_length])\n",
    "    ctc_model = keras.Model(inputs=[labels, inputs, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.0008, beta_1=0.9, beta_2=0.999, decay=0.01, epsilon=10e-8)\n",
    "    # ctc_model=multi_gpu_model(ctc_model,gpus=2)\n",
    "    ctc_model.compile(loss={'ctc': lambda y_true, output: output}, optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return base_model, ctc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置GPU内存按需分配\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1cxbqBf0e6Li"
   },
   "source": [
    "# 开始训练、测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7SKCjWxKQeSt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_inputs (InputLayer)         [(None, None, 64, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_1-1 (Conv2D)             (None, None, 64, 64) 640         the_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_1-1 (BatchNormaliza (None, None, 64, 64) 256         Conv2D_1-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_1-2 (Conv2D)             (None, None, 64, 64) 36928       BatchNormal_1-1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_1-2 (BatchNormaliza (None, None, 64, 64) 256         Conv2D_1-2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "MaxPooling2D_1 (MaxPooling2D)   (None, None, 32, 64) 0           BatchNormal_1-2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_2-1 (Conv2D)             (None, None, 32, 128 73856       MaxPooling2D_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_2-1 (BatchNormaliza (None, None, 32, 128 512         Conv2D_2-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_2-2 (Conv2D)             (None, None, 32, 128 147584      BatchNormal_2-1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_2-2 (BatchNormaliza (None, None, 32, 128 512         Conv2D_2-2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "MaxPooling2D_2 (MaxPooling2D)   (None, None, 16, 128 0           BatchNormal_2-2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_3-1 (Conv2D)             (None, None, 16, 256 295168      MaxPooling2D_2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_3-1 (BatchNormaliza (None, None, 16, 256 1024        Conv2D_3-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_3-2 (Conv2D)             (None, None, 16, 256 590080      BatchNormal_3-1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_3-2 (BatchNormaliza (None, None, 16, 256 1024        Conv2D_3-2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "MaxPooling2D_3 (MaxPooling2D)   (None, None, 8, 256) 0           BatchNormal_3-2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_4-1 (Conv2D)             (None, None, 8, 512) 1180160     MaxPooling2D_3[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_4-1 (BatchNormaliza (None, None, 8, 512) 2048        Conv2D_4-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Conv2D_4-2 (Conv2D)             (None, None, 8, 512) 2359808     BatchNormal_4-1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "BatchNormal_4-2 (BatchNormaliza (None, None, 8, 512) 2048        Conv2D_4-2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Reshape_1 (Reshape)             (None, None, 4096)   0           BatchNormal_4-2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm1 (LSTM)                    (None, None, 128)    2163200     Reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm2 (LSTM)                    (None, None, 256)    394240      lstm1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Dense_1 (Dense)                 (None, None, 512)    131584      lstm2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Dense_2 (Dense)                 (None, None, 7)      3591        Dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Activation_1 (Activation)       (None, None, 7)      0           Dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           the_labels[0][0]                 \n",
      "                                                                 Activation_1[0][0]               \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,384,519\n",
      "Trainable params: 7,380,679\n",
      "Non-trainable params: 3,840\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n",
      "44/44 [==============================] - 4s 39ms/step - loss: 4.2369 - accuracy: 0.0749 - val_loss: 3.5418 - val_accuracy: 0.0513\n",
      "Epoch 2/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.3289 - accuracy: 0.8482 - val_loss: 0.9080 - val_accuracy: 0.6731\n",
      "Epoch 3/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0558 - accuracy: 0.9788 - val_loss: 0.3134 - val_accuracy: 0.8397\n",
      "Epoch 4/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0138 - accuracy: 0.9985 - val_loss: 0.4222 - val_accuracy: 0.8974\n",
      "Epoch 5/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0351 - accuracy: 0.9852 - val_loss: 0.1468 - val_accuracy: 0.9359\n",
      "Epoch 6/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0059 - accuracy: 0.9988 - val_loss: 0.1463 - val_accuracy: 0.9295\n",
      "Epoch 7/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0865 - val_accuracy: 0.9423\n",
      "Epoch 8/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.0994 - val_accuracy: 0.9423\n",
      "Epoch 9/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0918 - val_accuracy: 0.9679\n",
      "Epoch 10/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9679\n",
      "Epoch 11/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0400 - val_accuracy: 0.9936\n",
      "Epoch 12/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 0.9936\n",
      "Epoch 13/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 0.9936\n",
      "Epoch 14/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0300 - val_accuracy: 0.9936\n",
      "Epoch 15/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0290 - val_accuracy: 0.9936\n",
      "Epoch 16/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 0.9936\n",
      "Epoch 17/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 9.6754e-04 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 0.9936\n",
      "Epoch 18/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 9.3766e-04 - accuracy: 1.0000 - val_loss: 0.0276 - val_accuracy: 0.9936\n",
      "Epoch 19/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 8.5711e-04 - accuracy: 1.0000 - val_loss: 0.0273 - val_accuracy: 0.9936\n",
      "Epoch 20/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 8.0154e-04 - accuracy: 1.0000 - val_loss: 0.0273 - val_accuracy: 0.9936\n",
      "Epoch 21/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 7.7082e-04 - accuracy: 1.0000 - val_loss: 0.0272 - val_accuracy: 0.9936\n",
      "Epoch 22/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 7.1317e-04 - accuracy: 1.0000 - val_loss: 0.0272 - val_accuracy: 0.9936\n",
      "Epoch 23/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 6.9535e-04 - accuracy: 1.0000 - val_loss: 0.0269 - val_accuracy: 0.9936\n",
      "Epoch 24/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 0.0012 - accuracy: 0.9993 - val_loss: 0.0398 - val_accuracy: 0.9936\n",
      "Epoch 25/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 7.4863e-04 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 0.9936\n",
      "Epoch 26/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 7.1045e-04 - accuracy: 1.0000 - val_loss: 0.0306 - val_accuracy: 0.9936\n",
      "Epoch 27/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 7.1261e-04 - accuracy: 1.0000 - val_loss: 0.0288 - val_accuracy: 0.9936\n",
      "Epoch 28/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 6.3663e-04 - accuracy: 1.0000 - val_loss: 0.0277 - val_accuracy: 0.9936\n",
      "Epoch 29/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 6.0920e-04 - accuracy: 1.0000 - val_loss: 0.0270 - val_accuracy: 0.9936\n",
      "Epoch 30/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 5.9396e-04 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 0.9936\n",
      "Epoch 31/1000\n",
      "44/44 [==============================] - 1s 26ms/step - loss: 5.5146e-04 - accuracy: 1.0000 - val_loss: 0.0260 - val_accuracy: 0.9936\n",
      "Epoch 00031: early stopping\n",
      "模型结构及权重已保存\n"
     ]
    }
   ],
   "source": [
    "image_size=64  \n",
    "batch_size = 16\n",
    "logs_path = '../logs'\n",
    "\n",
    "input_size=(None, image_size, 1)\n",
    "base_model, ctc_model = create_model(input_size, output_size=py_list_size) \n",
    "print(ctc_model.summary())\n",
    "\n",
    "# keras.utils.plot_model(base_model, show_shapes=True)\n",
    "\n",
    "if not os.path.exists(logs_path):  # 判断保存模型的目录是否存在\n",
    "    os.makedirs(logs_path)  # 如果不存在，就新建一个，避免之后保存模型的时候炸掉\n",
    "\n",
    "# train_batch_gen = get_batch_generator(batch_size, train_wav_file_list, train_label_list, py_list, image_size)\n",
    "# validate_batch_gen = get_batch_generator(batch_size, validate_wav_file_list, validate_label_list, py_list, image_size)\n",
    "# input_data = next(train_batch_gen)[0]\n",
    "# plt.imshow(input_data['the_inputs'][0].T[0])\n",
    "# plt.show()\n",
    "# print(input_data['the_inputs'].shape, input_data['the_labels'].shape, input_data['input_length'].shape, input_data['label_length'].shape)\n",
    "\n",
    "train_data = get_train_data(train_wav_file_list, train_label_list, py_list, image_size)\n",
    "\n",
    "cb = []\n",
    "cb.append(keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "# 当监测值不再改善时，该回调函数将中止训练可防止过拟合\n",
    "cb.append(keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, verbose=1, mode='auto'))\n",
    "# his = ctc_model.fit_generator(train_batch_gen, verbose=1, steps_per_epoch=train_file_nums//batch_size, validation_data=validate_batch_gen, validation_steps=validate_file_nums//batch_size, epochs=1000, callbacks=cb)  \n",
    "\n",
    "his = ctc_model.fit(train_data[0], train_data[1], validation_split=0.1, batch_size=32, epochs=1000, callbacks=cb)  # callback的epoch都是对fit里的参数来说\n",
    "\n",
    "#  保存模型结构及权重\n",
    "ctc_model.save_weights(r'save_weights.h5')\n",
    "with open(r'model_struct.json', 'w') as f:\n",
    "    json_string = base_model.to_json()\n",
    "    f.write(json_string)  # 保存模型信息\n",
    "print('模型结构及权重已保存')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "91F-ESvt91QL",
    "outputId": "15d77e50-ff58-46a0-9f97-4447c441babd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已加载\n",
      "py_list已加载\n",
      "trian:样本数260错误数0准确率：100.000000%\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-680cbd0f4ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trian'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_wavs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_wavs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-680cbd0f4ae6>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(kind, wavs, labels)\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0merror_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'真实标签：'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'预测结果'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}:样本数{}错误数{}准确率：{:%}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_cnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0merror_cnt\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdata_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# 训练集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# 加载权重\n",
    "with open(r'model_struct.json') as f:\n",
    "  model_struct = f.read()\n",
    "test_model = keras.models.model_from_json(model_struct)\n",
    "test_model.load_weights(r'save_weights.h5')\n",
    "# model = keras.models.load_model('all_model.h5')\n",
    "print('模型已加载')\n",
    "py_list = []\n",
    "with open(r'pinyin_list.txt', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "for line in contents:\n",
    "    i = line.strip('\\n')\n",
    "    py_list.append(i)\n",
    "print('py_list已加载')\n",
    "\n",
    "# 对模型进行评价\n",
    "def evaluate(kind, wavs, labels):\n",
    "  data_num = len(wavs)\n",
    "  error_cnt = 0\n",
    "  for i in range(data_num):\n",
    "    pre = test_model.predict(wavs[i]) # (1, 20, 11)\n",
    "    pre_index, pre_label = decode_ctc(pre, py_list) # ['5']\n",
    "    try:\n",
    "      pre_label = int(pre_label[0])\n",
    "    except:\n",
    "      pre_label = None\n",
    "    label = int(py_list[labels[i]])\n",
    "    if label != pre_label:\n",
    "      error_cnt += 1\n",
    "      print('真实标签：', label, '预测结果', pre_label)\n",
    "  print('{}:样本数{}错误数{}准确率：{:%}'.format(kind, data_num, error_cnt, (1-error_cnt/data_num)))\n",
    "\n",
    "# 训练集\n",
    "train_wavs, train_labels = get_test_data(image_size, train_wav_file_list, train_label_list, py_list)\n",
    "# 测试集\n",
    "test_wavs, test_labels = get_test_data(image_size, test_wav_file_list, test_label_list, py_list)\n",
    "\n",
    "evaluate('trian', train_wavs, train_labels)\n",
    "evaluate('test', test_wavs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0lXiT7eJVVv"
   },
   "source": [
    "# ---------------------------------------------------7.对比实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jj9RaRpj96lC"
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "dKaLi23oJVtl",
    "outputId": "2bd0c614-4eb9-4b74-c3cb-a298083cf871"
   },
   "outputs": [],
   "source": [
    "def create_cnn_model(input_size, output_size):\n",
    "    inputs = Input(name='the_inputs', shape=input_size)\n",
    "    # 1\n",
    "    h1_1 = Conv2D(64, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_1-1')(inputs)\n",
    "    h1_2 = BatchNormalization(name='BatchNormal_1-1')(h1_1)\n",
    "    h1_3 = Conv2D(64, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_1-2')(h1_2)\n",
    "    h1_4 = BatchNormalization(name='BatchNormal_1-2')(h1_3)\n",
    "    h1_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_1')(h1_4)\n",
    "    # 2\n",
    "    h2_1 = Conv2D(128, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_2-1')(h1_5)\n",
    "    h2_2 = BatchNormalization(name='BatchNormal_2-1')(h2_1)\n",
    "    h2_3 = Conv2D(128, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_2-2')(h2_2)\n",
    "    h2_4 = BatchNormalization(name='BatchNormal_2-2')(h2_3)\n",
    "    h2_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_2')(h2_4)\n",
    "    # 3\n",
    "    h3_1 = Conv2D(256, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_3-1')(h2_5)\n",
    "    h3_2 = BatchNormalization(name='BatchNormal_3-1')(h3_1)\n",
    "    h3_3 = Conv2D(256, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_3-2')(h3_2)\n",
    "    h3_4 = BatchNormalization(name='BatchNormal_3-2')(h3_3)\n",
    "    h3_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_3')(h3_4)\n",
    "    # 4\n",
    "    h4_1 = Conv2D(521, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_4-1')(h3_5)\n",
    "    h4_2 = BatchNormalization(name='BatchNormal_4-1')(h4_1)\n",
    "    h4_3 = Conv2D(512, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_4-2')(h4_2)\n",
    "    h4_4 = BatchNormalization(name='BatchNormal_4-2')(h4_3)\n",
    "    # 由于声学模型网络结构原因（3个maxpooling层），我们的音频数据的每个维度需要能够被8整除。这里输入序列经过卷积网络后，长度缩短了8倍，因此我们训练实际输入的数据为wav_len//8。\n",
    "    h5_1 = Reshape((-1, int(input_size[1]//8*512)), name='Reshape_1')(h4_4)\n",
    "    h5_2 = Dense(512, activation='relu', use_bias=True, kernel_initializer='he_normal', name='Dense_1')(h5_1)\n",
    "    h5_3 = Dense(output_size, activation=\"relu\", use_bias=True, kernel_initializer='he_normal', name='Dense_2')(h5_2)#(layer_h15)\n",
    "\n",
    "    outputs = Activation('softmax', name='Activation_1')(h5_3)\n",
    "\n",
    "    base_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    labels = Input(name='the_labels', shape=[None], dtype='float32')\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "    # keras.Lambda(function, output_shape=None, mask=None, arguments=None)\n",
    "    # 将任意表达式封装为 Layer 对象\n",
    "    loss_out = Lambda(ctc_lambda, output_shape=(1,), name='ctc')([labels, outputs, input_length, label_length])\n",
    "    ctc_model = keras.Model(inputs=[labels, inputs, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.0008, beta_1=0.9, beta_2=0.999, decay=0.01, epsilon=10e-8)\n",
    "    # ctc_model=multi_gpu_model(ctc_model,gpus=2)\n",
    "    ctc_model.compile(loss={'ctc': lambda y_true, output: output}, optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return base_model, ctc_model\n",
    "base_cnn_model, ctc_cnn_model = create_cnn_model(input_size, output_size=py_list_size) \n",
    "\n",
    "his = ctc_cnn_model.fit(train_data[0], train_data[1], validation_split=0.1, batch_size=32, verbose=0, epochs=1000, callbacks=cb)  # callback的epoch都是对fit里的参数来说\n",
    "\n",
    "#  保存模型结构及权重\n",
    "ctc_cnn_model.save_weights(r'save_cnn_weights.h5')\n",
    "with open(r'model_cnn_struct.json', 'w') as f:\n",
    "    json_string = base_cnn_model.to_json()\n",
    "    f.write(json_string)  # 保存模型信息\n",
    "print('模型结构及权重已保存')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "uWtp6s_89_WL",
    "outputId": "d00daf27-3ec8-4bfb-95ce-696f1757a2dc"
   },
   "outputs": [],
   "source": [
    "# 加载权重\n",
    "with open(r'model_cnn_struct.json') as f:\n",
    "  model_cnn_struct = f.read()\n",
    "test_cnn_model = keras.models.model_from_json(model_cnn_struct)\n",
    "# model.summary()\n",
    "test_cnn_model.load_weights(r'save_cnn_weights.h5')\n",
    "# model = keras.models.load_model('all_model.h5')\n",
    "print('模型已加载')\n",
    "py_list = []\n",
    "with open(r'pinyin_list.txt', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "for line in contents:\n",
    "    i = line.strip('\\n')\n",
    "    py_list.append(i)\n",
    "print('py_list已加载')\n",
    "\n",
    "# 对模型进行评价\n",
    "def evaluate(kind, wavs, labels):\n",
    "  data_num = len(wavs)\n",
    "  error_cnt = 0\n",
    "  for i in range(data_num):\n",
    "    pre = test_cnn_model.predict(wavs[i]) # (1, 20, 11)\n",
    "    pre_index, pre_label = decode_ctc(pre, py_list) # ['5']\n",
    "    try:\n",
    "      pre_label = int(pre_label[0])\n",
    "    except:\n",
    "      pre_label = None\n",
    "    label = int(py_list[labels[i]])\n",
    "    if label != pre_label:\n",
    "      error_cnt += 1\n",
    "      print('真实标签：', label, '预测结果', pre_label)\n",
    "  print('{}:样本数{}错误数{}准确率：{:%}'.format(kind, data_num, error_cnt, (1-error_cnt/data_num)))\n",
    "\n",
    "evaluate('trian', train_wavs, train_labels)\n",
    "evaluate('test', test_wavs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zescE9wJ-BtP"
   },
   "source": [
    "#LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8zL0RHpIjEJk"
   },
   "outputs": [],
   "source": [
    "# #LSTM minist分类\n",
    "# from keras.datasets import mnist\n",
    "# n_input = 28\n",
    "# n_step = 28\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# x_train = x_train.reshape(-1, n_step, n_input, 1)\n",
    "# x_test = x_test.reshape(-1, n_step, n_input, 1)\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "\n",
    "# y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, n_classes)\n",
    "# inputs = Input(name='the_inputs', shape=(n_step, n_input, 1))\n",
    "# inner = Reshape((n_step, n_input),name='Reshape_1')(inputs)\n",
    "# # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "# lstm_1 = LSTM(128, kernel_initializer='he_normal', name='lstm1')(inner)\n",
    "# d1 = Dense(10)(lstm_1)\n",
    "# outputs = Activation('softmax')(d1)\n",
    "# model = keras.Model(inputs, outputs)\n",
    "\n",
    "# model.summary()\n",
    "# model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# model.fit(x_train, y_train,batch_size=128,epochs=20,verbose=1,validation_data=(x_test, y_test))\n",
    "# scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print('LSTM test score:', scores[0])\n",
    "# print('LSTM test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "hF_MVb2fIlbx",
    "outputId": "57000e60-0043-416a-cc49-e1607b1f4e57"
   },
   "outputs": [],
   "source": [
    "# LSTM太差\n",
    "def create_lstm_model(input_size, output_size):\n",
    "    inputs = Input(name='the_inputs', shape=input_size)\n",
    "    inner = Reshape((72, 64),name='Reshape_1')(inputs)\n",
    "    # expected input data shape: (batch_size, timesteps, data_dim)\n",
    "    lstm_1 = Bidirectional(LSTM(32, return_sequences=True, kernel_initializer='he_normal', name='lstm1'))(inner)\n",
    "    \n",
    "    lstm_2 = Bidirectional(LSTM(128, return_sequences=True, kernel_initializer='he_normal', name='lstm3'))(lstm_1)\n",
    "\n",
    "    \n",
    "    h5_2 = Dense(512, activation='relu', use_bias=True, kernel_initializer='he_normal', name='Dense_4')(lstm_2)\n",
    "    h5_3 = Dense(output_size, activation=\"relu\", use_bias=True, kernel_initializer='he_normal', name='Dense_5')(h5_2)#(layer_h15)\n",
    "\n",
    "    outputs = Activation('softmax', name='Activation_1')(h5_3)\n",
    "\n",
    "    base_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    labels = Input(name='the_labels', shape=[None], dtype='float32')\n",
    "    input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "    label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "    # keras.Lambda(function, output_shape=None, mask=None, arguments=None)\n",
    "    # 将任意表达式封装为 Layer 对象\n",
    "    loss_out = Lambda(ctc_lambda, output_shape=(1,), name='ctc')([labels, outputs, input_length, label_length])\n",
    "    ctc_model = keras.Model(inputs=[labels, inputs, input_length, label_length], outputs=loss_out)\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.0008, beta_1=0.9, beta_2=0.999, decay=0.01, epsilon=10e-8)\n",
    "    # ctc_model=multi_gpu_model(ctc_model,gpus=2)\n",
    "    ctc_model.compile(loss={'ctc': lambda y_true, output: output}, optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return base_model, ctc_model\n",
    "\n",
    "base_lstm_model, ctc_lstm_model = create_lstm_model(input_size=(72, 64, 1), output_size=py_list_size) \n",
    "#ctc_lstm_model.summary()\n",
    "train_data = get_train_data(train_wav_file_list, train_label_list, py_list, image_size) \n",
    "his = ctc_lstm_model.fit(train_data[0], train_data[1], validation_split=0.1, batch_size=32, verbose=0, epochs=1000, callbacks=cb)  # callback的epoch都是对fit里的参数来说\n",
    "\n",
    "#  保存模型结构及权重\n",
    "ctc_lstm_model.save_weights(r'save_lstm_weights.h5')\n",
    "with open(r'model_lstm_struct.json', 'w') as f:\n",
    "    json_string = base_lstm_model.to_json()\n",
    "    f.write(json_string)  # 保存模型信息\n",
    "print('模型结构及权重已保存')\n",
    "\n",
    "# 加载权重\n",
    "with open(r'model_lstm_struct.json') as f:\n",
    "  model_lstm_struct = f.read()\n",
    "test_lstm_model = keras.models.model_from_json(model_lstm_struct)\n",
    "\n",
    "test_lstm_model.load_weights(r'save_lstm_weights.h5')\n",
    "print('模型已加载')\n",
    "py_list = []\n",
    "with open(r'pinyin_list.txt', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "for line in contents:\n",
    "    i = line.strip('\\n')\n",
    "    py_list.append(i)\n",
    "print('py_list已加载')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AstalecH-Rod",
    "outputId": "357d6ae8-9a68-4ed5-f52b-cbfb24887f74"
   },
   "outputs": [],
   "source": [
    "# 对模型进行评价\n",
    "def evaluate(kind, wavs, labels):\n",
    "  data_num = len(wavs)\n",
    "  error_cnt = 0\n",
    "  for i in range(data_num):\n",
    "    pre = test_lstm_model.predict(wavs[i]) # (1, 20, 11)\n",
    "    pre_index, pre_label = decode_ctc(pre, py_list) # ['5']\n",
    "    try:\n",
    "      pre_label = int(pre_label[0])\n",
    "    except:\n",
    "      pre_label = None\n",
    "    label = int(py_list[labels[i]])\n",
    "    if label != pre_label:\n",
    "      error_cnt += 1\n",
    "      print('真实标签：', label, '预测结果', pre_label)\n",
    "  print('{}:样本数{}错误数{}准确率：{:%}'.format(kind, data_num, error_cnt, (1-error_cnt/data_num)))\n",
    "\n",
    "# 训练集\n",
    "train_wavs, train_labels = get_test_data(image_size, train_wav_file_list, train_label_list, py_list)\n",
    "# 测试集\n",
    "test_wavs, test_labels = get_test_data(image_size, test_wav_file_list, test_label_list, py_list)\n",
    "\n",
    "evaluate('trian', train_wavs, train_labels)\n",
    "evaluate('test', test_wavs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJen5_JX-UOn"
   },
   "source": [
    "# 交叉熵损失函数的CRNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "NZd7lxJiJ7Av",
    "outputId": "dafd9604-2c5b-487e-f555-fdd181e174c7"
   },
   "outputs": [],
   "source": [
    "def create_cross_model(input_size, output_size):\n",
    "    inputs = Input(name='the_inputs', shape=input_size)\n",
    "    # 1\n",
    "    h1_1 = Conv2D(64, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_1-1')(inputs)\n",
    "    h1_2 = BatchNormalization(name='BatchNormal_1-1')(h1_1)\n",
    "    h1_3 = Conv2D(64, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_1-2')(h1_2)\n",
    "    h1_4 = BatchNormalization(name='BatchNormal_1-2')(h1_3)\n",
    "    h1_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_1')(h1_4)\n",
    "    # 2\n",
    "    h2_1 = Conv2D(128, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_2-1')(h1_5)\n",
    "    h2_2 = BatchNormalization(name='BatchNormal_2-1')(h2_1)\n",
    "    h2_3 = Conv2D(128, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_2-2')(h2_2)\n",
    "    h2_4 = BatchNormalization(name='BatchNormal_2-2')(h2_3)\n",
    "    h2_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_2')(h2_4)\n",
    "    # 3\n",
    "    h3_1 = Conv2D(256, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_3-1')(h2_5)\n",
    "    h3_2 = BatchNormalization(name='BatchNormal_3-1')(h3_1)\n",
    "    h3_3 = Conv2D(256, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_3-2')(h3_2)\n",
    "    h3_4 = BatchNormalization(name='BatchNormal_3-2')(h3_3)\n",
    "    h3_5 = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\", name='MaxPooling2D_3')(h3_4)\n",
    "    # 4\n",
    "    h4_1 = Conv2D(512, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_4-1')(h3_5)\n",
    "    h4_2 = BatchNormalization(name='BatchNormal_4-1')(h4_1)\n",
    "    h4_3 = Conv2D(512, (3, 3), use_bias=True, activation='relu', padding='same', kernel_initializer='he_normal', name='Conv2D_4-2')(h4_2)\n",
    "    h4_4 = BatchNormalization(name='BatchNormal_4-2')(h4_3)\n",
    "    # 由于声学模型网络结构原因（3个maxpooling层），我们的音频数据的每个维度需要能够被8整除。这里输入序列经过卷积网络后，长度缩短了8倍，因此我们训练实际输入的数据为wav_len//8。\n",
    "    h5_1 = Reshape((9, int(input_size[1]//8*512)), name='Reshape_1')(h4_4)\n",
    "\n",
    "    lstm_1 = LSTM(128, return_sequences=True, kernel_initializer='he_normal', name='lstm1')(h5_1)\n",
    "    lstm_2 = LSTM(256, return_sequences=True, kernel_initializer='he_normal', name='lstm2')(lstm_1)\n",
    "    h5_11 = Flatten()(lstm_2)\n",
    "    h5_2 = Dense(512, activation='relu', use_bias=True, kernel_initializer='he_normal', name='Dense_1')(h5_11)\n",
    "    h5_3 = Dense(output_size, activation=\"relu\", use_bias=True, kernel_initializer='he_normal', name='Dense_2')(h5_2)#(layer_h15)\n",
    "    outputs = Activation('softmax', name='Activation_1')(h5_3)\n",
    "    base_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.09, beta_2=0.999, decay=0.1, epsilon=10e-8)\n",
    "    # ctc_model=multi_gpu_model(ctc_model,gpus=2)\n",
    "    base_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    return base_model\n",
    "\n",
    "crnn_cross_model = create_cross_model(input_size=(72, 64, 1), output_size=6) \n",
    "\n",
    "train_data = get_train_data(train_wav_file_list, train_label_list, py_list, image_size) \n",
    "from keras.utils import np_utils\n",
    "\n",
    "l = np_utils.to_categorical(train_data[0]['the_labels'], num_classes=6)\n",
    "his = crnn_cross_model.fit(train_data[0]['the_inputs'], l, validation_split=0.2, batch_size=32, verbose=0, epochs=1000, callbacks=cb)  # callback的epoch都是对fit里的参数来说\n",
    "\n",
    "#  保存模型结构及权重\n",
    "crnn_cross_model.save_weights(r'save_crnn_cross_weights.h5')\n",
    "with open(r'model_crnn_cross_struct.json', 'w') as f:\n",
    "    json_string = crnn_cross_model.to_json()\n",
    "    f.write(json_string)  # 保存模型信息\n",
    "print('模型结构及权重已保存')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "QEJf_dDx-bNA",
    "outputId": "695075fd-976c-438c-c3c5-2ef1c9630f95"
   },
   "outputs": [],
   "source": [
    "# 加载权重\n",
    "with open(r'model_crnn_cross_struct.json') as f:\n",
    "  crnn_cross_struct = f.read()\n",
    "test_crnn_cross_model = keras.models.model_from_json(crnn_cross_struct)\n",
    "test_crnn_cross_model.load_weights(r'save_crnn_cross_weights.h5')\n",
    "print('模型已加载')\n",
    "py_list = []\n",
    "with open(r'pinyin_list.txt', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "for line in contents:\n",
    "    i = line.strip('\\n')\n",
    "    py_list.append(i)\n",
    "print('py_list已加载')\n",
    "loss, accuracy = crnn_cross_model.evaluate(np.array(test_wavs).reshape((60, 72, 64, 1)), np_utils.to_categorical(np.array(test_labels),num_classes=6))\n",
    "\n",
    "print('准确率：{:%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AyHxP5DpwGje"
   },
   "outputs": [],
   "source": [
    "# shown_offset = re.findall(re.compile(r'<ul.*?id=\"feedlist_id\" shown-offset=\"(.*?)\">', re.S), response.text)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OD_65MloWC7L"
   },
   "source": [
    "# 选择方差第二大的子载波"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XjMy46Gcora2",
    "outputId": "3d176941-de39-4166-debb-83dabd02c41d"
   },
   "outputs": [],
   "source": [
    "def process_data(raw_file_path, save_path):\n",
    "  X={}\n",
    "  for f in tqdm(os.listdir(raw_file_path)):\n",
    "    if f.endswith('.dat'):\n",
    "      file_name = os.path.join(raw_file_path, f)\n",
    "      extracted_data = extract_csi(file_name)\n",
    "      print('processing {} the length of this file is:{}'.format(file_name, len(extracted_data)))\n",
    "      tx, rx, sub = extracted_data[0]['csi'].shape\n",
    "      data_csi = np.zeros((len(extracted_data), tx, rx, sub), dtype=np.complex64)\n",
    "      for i in range(len(extracted_data)):\n",
    "        data_csi[i] = get_scaled_csi(extracted_data[i])\n",
    "      data_csi = np.clip(np.abs(np.squeeze(data_csi)), 1e-8, 1e100)[:,:,:2,:].reshape(-1, 120)   # (205, 2, 2, 30)\n",
    "      data = np.zeros((data_csi.shape[0],120)) \n",
    "      var = [] \n",
    "      for i in range(120):\n",
    "        data_csi_sub = data_csi[:, i]  \n",
    "        b, a = signal.butter(5, 4*2/30, 'low')\n",
    "        carrier_data = signal.lfilter(b, a, data_csi_sub) # N*1\n",
    "        data[:, i] = carrier_data\n",
    "        \n",
    "        # length = len(carrier_data)\n",
    "        # var_temp = np.var(carrier_data[length//5:3*length//5]) \n",
    "        # var.append(var_temp)\n",
    "      \n",
    "      #data = data_csi[:, np.argsort(var)[1]]\n",
    "      #print(np.argsort(var)[1])\n",
    "      scio.savemat(os.path.join(save_path, f.split('.')[0]+'.mat'), {'csi': data})\n",
    "      X[f]=data\n",
    "  print('all raw file processed')\n",
    "  return X\n",
    "\n",
    "train_raw_file_path = r'/content/drive/My Drive/SR_CSI/Gestures_data/train'\n",
    "train_save_path = r'/content/drive/My Drive/SR_CSI/Gestures_data/train'\n",
    "X_train = process_data(train_raw_file_path, train_save_path)\n",
    "test_raw_file_path = r'/content/drive/My Drive/SR_CSI/Gestures_data/test'\n",
    "test_save_path = r'/content/drive/My Drive/SR_CSI//Gestures_data/test'\n",
    "X_test = process_data(test_raw_file_path, test_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "pw0BFfg92Vxv",
    "outputId": "1da292ff-4b7a-4eec-fa03-f1e6ef42a758"
   },
   "outputs": [],
   "source": [
    "# 重新构造mat文件选择最佳子载波上的CSI\n",
    "best_index_list = [58, 1, 29, 29, 28, 29]\n",
    "\n",
    "#\n",
    "for f in tqdm(os.listdir(train_save_path)):\n",
    "    if f.endswith('.mat'):\n",
    "      file_name = os.path.join(train_save_path, f)\n",
    "      activate_index = int(re.findall(re.compile(r'csi-s1-e1-a(.*?)-.*?.mat', re.S), f)[0])\n",
    "      csi = np.squeeze(scio.loadmat(file_name)['csi'])\n",
    "      best_csi = csi[:, best_index_list[activate_index-1]]     \n",
    "      scio.savemat(file_name, {'csi': best_csi})\n",
    "for f in tqdm(os.listdir(test_save_path)):\n",
    "    if f.endswith('.mat'):\n",
    "      file_name = os.path.join(test_save_path, f)\n",
    "      activate_index = int(re.findall(re.compile(r'csi-s1-e1-a(.*?)-.*?.mat', re.S), f)[0])\n",
    "      best_csi = np.squeeze(scio.loadmat(file_name)['csi'])[:, best_index_list[activate_index-1]]\n",
    "      scio.savemat(file_name, {'csi': best_csi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "YebtHSJ4VBn8",
    "outputId": "08535eb4-e3aa-4cba-dfb0-cb5bc70fbd84"
   },
   "outputs": [],
   "source": [
    "train_file_path = train_save_path\n",
    "train_label_file_list, train_wav_file_list = get_file_list(train_file_path)\n",
    "\n",
    "train_label_list = get_label_data(train_label_file_list)\n",
    "\n",
    "# 用迭代器的时候需要\n",
    "# train_wav_file_list, validate_wav_file_list, train_label_list, validate_label_list = train_test_split(train_wav_file_list, train_label_list, test_size=0.2, random_state=0)\n",
    "\n",
    "# 每个文件拼音标签的集合\n",
    "with open('label_list.txt', 'w') as f:  \n",
    "    f.write('\\n'.join(train_label_list))  \n",
    "py_list = gen_py_list(train_label_list)  # 拼音的集合\n",
    "\n",
    "with open('pinyin_list.txt', 'w') as f:\n",
    "    f.write('\\n'.join(py_list))  # 保存拼音列表\n",
    "\n",
    "train_file_nums = len(train_wav_file_list)\n",
    "# validate_file_nums = len(validate_wav_file_list)\n",
    "\n",
    "py_list_size = len(py_list)  # 模型输出的维度\n",
    "print('py_list:', py_list)\n",
    "print('train files amount:', len(train_label_list), train_file_nums, 'label amount:', py_list_size)#, 'validate files amount:', len(validate_label_list), validate_file_nums,)\n",
    "\n",
    "# 测试数据\n",
    "test_file_path = test_save_path # r'/content/drive/My Drive/data_thchs30'\n",
    "test_label_file_list, test_wav_file_list = get_file_list(test_file_path)\n",
    "\n",
    "test_label_list = get_label_data(test_label_file_list)\n",
    "print('test files amount:',len(test_label_file_list), len(test_wav_file_list))\n",
    "test_data_num = len(test_label_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "R0vC4Ji3Wdbf",
    "outputId": "96cdb91b-9c1d-4bd4-ee57-c62d4c4fdfec"
   },
   "outputs": [],
   "source": [
    "base_sec_model, ctc_sec_model = create_model(input_size, output_size=py_list_size) \n",
    "\n",
    "train_data = get_train_data(train_wav_file_list, train_label_list, py_list, image_size)\n",
    "\n",
    "\n",
    "cb = []\n",
    "cb.append(keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0))\n",
    "# 当监测值不再改善时，该回调函数将中止训练可防止过拟合\n",
    "cb.append(keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, verbose=1, mode='auto'))\n",
    "# his = ctc_model.fit_generator(train_batch_gen, verbose=1, steps_per_epoch=train_file_nums//batch_size, validation_data=validate_batch_gen, validation_steps=validate_file_nums//batch_size, epochs=1000, callbacks=cb)  \n",
    "\n",
    "\n",
    "his = ctc_sec_model.fit(train_data[0], train_data[1], validation_split=0.1, batch_size=32, verbose=0, epochs=1000, callbacks=cb)  # callback的epoch都是对fit里的参数来说\n",
    "\n",
    "#  保存模型结构及权重\n",
    "ctc_sec_model.save_weights(r'save_sec_weights.h5')\n",
    "with open(r'model_sec_struct.json', 'w') as f:\n",
    "    json_string = base_sec_model.to_json()\n",
    "    f.write(json_string)  # 保存模型信息\n",
    "print('模型结构及权重已保存')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "GYzx9TuLWZDV",
    "outputId": "8d9ce035-e20a-4b69-b97f-e2e41f8c86e5"
   },
   "outputs": [],
   "source": [
    "# 加载权重\n",
    "with open(r'model_sec_struct.json') as f:\n",
    "  model_struct = f.read()\n",
    "test_sec_model = keras.models.model_from_json(model_struct)\n",
    "test_sec_model.load_weights(r'save_sec_weights.h5')\n",
    "# model = keras.models.load_model('all_model.h5')\n",
    "print('模型已加载')\n",
    "py_list = []\n",
    "with open(r'pinyin_list.txt', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "for line in contents:\n",
    "    i = line.strip('\\n')\n",
    "    py_list.append(i)\n",
    "print('py_list已加载')\n",
    "\n",
    "# 对模型进行评价\n",
    "def evaluate(kind, wavs, labels):\n",
    "  data_num = len(wavs)\n",
    "  error_cnt = 0\n",
    "  for i in range(data_num):\n",
    "    pre = test_sec_model.predict(wavs[i]) # (1, 20, 11)\n",
    "    pre_index, pre_label = decode_ctc(pre, py_list) # ['5']\n",
    "    try:\n",
    "      pre_label = int(pre_label[0])\n",
    "    except:\n",
    "      pre_label = None\n",
    "    label = int(py_list[labels[i]])\n",
    "    if label != pre_label:\n",
    "      error_cnt += 1\n",
    "      print('真实标签：', label, '预测结果', pre_label)\n",
    "  print('{}:样本数{}错误数{}准确率：{:%}'.format(kind, data_num, error_cnt, (1-error_cnt/data_num)))\n",
    "\n",
    "# 训练集\n",
    "train_wavs, train_labels = get_test_data(image_size, train_wav_file_list, train_label_list, py_list)\n",
    "# 测试集\n",
    "test_wavs, test_labels = get_test_data(image_size, test_wav_file_list, test_label_list, py_list)\n",
    "\n",
    "evaluate('trian', train_wavs, train_labels)\n",
    "evaluate('test', test_wavs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTFVTpBCXNs1"
   },
   "source": [
    "# 所有子载波求平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "cf4NJzVoTw94",
    "outputId": "dcb07260-fb9a-4be4-80e9-ce8ae7d0410b"
   },
   "outputs": [],
   "source": [
    "def process_data(raw_file_path, save_path):\n",
    "  X={}\n",
    "  for f in tqdm(os.listdir(raw_file_path)):\n",
    "    if f.endswith('.dat'):\n",
    "      file_name = os.path.join(raw_file_path, f)\n",
    "      extracted_data = extract_csi(file_name)\n",
    "      print('processing {} the length of this file is:{}'.format(file_name, len(extracted_data)))\n",
    "      tx, rx, sub = extracted_data[0]['csi'].shape\n",
    "      data_csi = np.zeros((len(extracted_data), tx, rx, sub), dtype=np.complex64)\n",
    "      for i in range(len(extracted_data)):\n",
    "        data_csi[i] = get_scaled_csi(extracted_data[i])\n",
    "      data_csi = np.clip(np.abs(np.squeeze(data_csi)), 1e-8, 1e100)[:,:,:2,:].reshape(-1, 4, 30)   # (205, 2, 2, 30)\n",
    "      data = []\n",
    "      for i in range(4):\n",
    "        data_ave = np.average(data_csi[:, i, :],axis=1)\n",
    "        data.extend(data_ave)\n",
    "      scio.savemat(os.path.join(save_path, f.split('.')[0]+'.mat'), {'csi': data})\n",
    "      X[f]=data\n",
    "  print('all raw file processed')\n",
    "  return X\n",
    "\n",
    "train_raw_file_path = r'/content/drive/My Drive/SR_CSI/Gestures_data/train'\n",
    "train_save_path = r'/content/drive/My Drive/SR_CSI/Gestures_data/train'\n",
    "X_train = process_data(train_raw_file_path, train_save_path)\n",
    "test_raw_file_path = r'/content/drive/My Drive/SR_CSI/Gestures_data/test'\n",
    "test_save_path = r'/content/drive/My Drive/SR_CSI//Gestures_data/test'\n",
    "X_test = process_data(test_raw_file_path, test_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "YesArgHvXc2O",
    "outputId": "712bf146-e937-4b33-9489-3906b5521aca"
   },
   "outputs": [],
   "source": [
    "train_file_path = train_save_path\n",
    "train_label_file_list, train_wav_file_list = get_file_list(train_file_path)\n",
    "\n",
    "train_label_list = get_label_data(train_label_file_list)\n",
    "\n",
    "# 用迭代器的时候需要\n",
    "# train_wav_file_list, validate_wav_file_list, train_label_list, validate_label_list = train_test_split(train_wav_file_list, train_label_list, test_size=0.2, random_state=0)\n",
    "\n",
    "# 每个文件拼音标签的集合\n",
    "with open('label_list.txt', 'w') as f:  \n",
    "    f.write('\\n'.join(train_label_list))  \n",
    "py_list = gen_py_list(train_label_list)  # 拼音的集合\n",
    "\n",
    "with open('pinyin_list.txt', 'w') as f:\n",
    "    f.write('\\n'.join(py_list))  # 保存拼音列表\n",
    "\n",
    "train_file_nums = len(train_wav_file_list)\n",
    "# validate_file_nums = len(validate_wav_file_list)\n",
    "\n",
    "py_list_size = len(py_list)  # 模型输出的维度\n",
    "print('py_list:', py_list)\n",
    "print('train files amount:', len(train_label_list), train_file_nums, 'label amount:', py_list_size)#, 'validate files amount:', len(validate_label_list), validate_file_nums,)\n",
    "\n",
    "# 测试数据\n",
    "test_file_path = test_save_path # r'/content/drive/My Drive/data_thchs30'\n",
    "test_label_file_list, test_wav_file_list = get_file_list(test_file_path)\n",
    "\n",
    "test_label_list = get_label_data(test_label_file_list)\n",
    "print('test files amount:',len(test_label_file_list), len(test_wav_file_list))\n",
    "test_data_num = len(test_label_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7P0aTudcXhWc"
   },
   "outputs": [],
   "source": [
    "base_ave_model, ctc_ave_model = create_model(input_size=(None, 256, 1), output_size=py_list_size) \n",
    "\n",
    "train_data = get_train_data(train_wav_file_list, train_label_list, py_list, image_size=256)\n",
    "\n",
    "his = ctc_ave_model.fit(train_data[0], train_data[1], validation_split=0.1, batch_size=32, verbose=1, epochs=1000, callbacks=cb)  # callback的epoch都是对fit里的参数来说\n",
    "\n",
    "#  保存模型结构及权重\n",
    "ctc_ave_model.save_weights(r'save_ave_weights.h5')\n",
    "with open(r'model_ave_struct.json', 'w') as f:\n",
    "    json_string = base_ave_model.to_json()\n",
    "    f.write(json_string)  # 保存模型信息\n",
    "print('模型结构及权重已保存')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZRuz4mZDXzBv",
    "outputId": "d41cf82a-9465-447e-beba-c1987ff10ee6"
   },
   "outputs": [],
   "source": [
    "# 加载权重\n",
    "with open(r'model_ave_struct.json') as f:\n",
    "  model_struct = f.read()\n",
    "test_ave_model = keras.models.model_from_json(model_struct)\n",
    "test_ave_model.load_weights(r'save_ave_weights.h5')\n",
    "# model = keras.models.load_model('all_model.h5')\n",
    "print('模型已加载')\n",
    "py_list = []\n",
    "with open(r'pinyin_list.txt', 'r') as f:\n",
    "    contents = f.readlines()\n",
    "for line in contents:\n",
    "    i = line.strip('\\n')\n",
    "    py_list.append(i)\n",
    "print('py_list已加载')\n",
    "\n",
    "# 对模型进行评价\n",
    "def evaluate(kind, wavs, labels):\n",
    "  data_num = len(wavs)\n",
    "  error_cnt = 0\n",
    "  for i in range(data_num):\n",
    "    pre = test_sec_model.predict(wavs[i]) # (1, 20, 11)\n",
    "    pre_index, pre_label = decode_ctc(pre, py_list) # ['5']\n",
    "    try:\n",
    "      pre_label = int(pre_label[0])\n",
    "    except:\n",
    "      pre_label = None\n",
    "    label = int(py_list[labels[i]])\n",
    "    if label != pre_label:\n",
    "      error_cnt += 1\n",
    "      print('真实标签：', label, '预测结果', pre_label)\n",
    "  print('{}:样本数{}错误数{}准确率：{:%}'.format(kind, data_num, error_cnt, (1-error_cnt/data_num)))\n",
    "\n",
    "# 训练集\n",
    "train_wavs, train_labels = get_test_data(image_size, train_wav_file_list, train_label_list, py_list)\n",
    "# 测试集\n",
    "test_wavs, test_labels = get_test_data(image_size, test_wav_file_list, test_label_list, py_list)\n",
    "\n",
    "evaluate('trian', train_wavs, train_labels)\n",
    "evaluate('test', test_wavs, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xm6T5vgpe6Ma"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X559TQs8e6NF"
   },
   "source": [
    "#### ROC、AUC曲线 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FuQricFRe6NH"
   },
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.preprocessing import label_binarize\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# import numpy as np\n",
    "# from scipy import interp\n",
    "# import matplotlib.pyplot as plt\n",
    "# from itertools import cycle\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# # 标签共三类\n",
    "# n_classes = py_list_size-1\n",
    "\n",
    "# X, y = make_classification(n_samples=80000, n_features=20, n_informative=3, n_redundant=0, n_classes=n_classes,\n",
    "#     n_clusters_per_class=2)\n",
    "# # print(X.shape, y.shape)\n",
    "# # print(X[0], y[0])\n",
    "# # (80000, 20) (80000,)\n",
    "# # [-1.90920853 -1.30052757 -0.76903467 -3.2546519  -0.02947816  0.14105006\n",
    "# #   0.43556031 -0.81300607 -0.94553296 -0.92774495  1.49041451 -0.4443121\n",
    "# #  -1.16342165 -0.32997815 -1.02907045 -0.39950447 -0.711287    0.51382424\n",
    "# #   2.88822258 -2.0935274 ] \n",
    "# # 1\n",
    "\n",
    "# # Binarize the output相当于one_hot\n",
    "# y = label_binarize(y, classes=[0, 1, 2])\n",
    "# # print(y.shape, y[0])\n",
    "# # (80000, 3) [0 1 0]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "# model = Sequential()\n",
    "# model.add(Dense(20, input_dim=20, activation='relu'))\n",
    "# model.add(Dense(40, activation='relu'))\n",
    "# model.add(Dense(3, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# model.fit(X_train, y_train, epochs=1, batch_size=100, verbose=1)\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "# # print(y_pred.shape)\n",
    "# # (40000, 3)\n",
    "\n",
    "# # Compute ROC curve and ROC area for each class\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "# for i in range(n_classes):\n",
    "#     # scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "#     # fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n",
    "#     # y 就是标准值，scores 是每个预测值对应的阳性概率，比如0.1就是指第一个数预测为阳性的概率为0.1，很显然，\n",
    "#     # y 和 socres应该有相同多的元素，都等于样本数。pos_label=2 是指在y中标签为2的是标准阳性标签，其余值是阴性。\n",
    "#     # 接下来选取一个阈值计算TPR/FPR,阈值的选取规则是在scores值中从大到小的以此选取，于是第一个选取的阈值是0.8\n",
    "#     # label=[1,1,2,2] scores=[0.1,0.4,0.35,0.8] thresholds=[0.8,0.4,0.35,0.1] 以threshold为0.8为例，将0.8与\n",
    "#     # scores 中所有值比较大小得到预测值，[0,0,0,1].对于label中两个1，其概率分别为0.1，0.4，小于阈值0.8，判定为\n",
    "#     # 负样本，而他们的label是1，说明他们确实是负样本，判断正确，是两个TN；两个2，对应概率为0.35，0.8，0.35小于\n",
    "#     # 0.8，判定为负样本，但是label是2，应该是个正样本，所以这是个FN；最后0.8>=0.8,这是个TP，所以最后的结果是\n",
    "#     # ：1个TP，2个TN，1个FN，0个FP\n",
    "#     fpr[i], tpr[i], thresholds = roc_curve(y_test[:, i], y_pred[:, i])  # (40000,)\n",
    "#     # print(fpr[i].shape)# (5491,)# (6562,)# (4271,)\n",
    "#     roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "\n",
    "# # 计算microROC曲线和ROC面积 \n",
    "# # .ravel()将多维数组转换为一维数组\n",
    "# fpr[\"micro\"], tpr[\"micro\"]  , thresholds = roc_curve(y_test.ravel(), y_pred.ravel())  #  (120000,)\n",
    "# roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# # 计算macroROC曲线和ROC面积\n",
    "# # 首先，汇总所有的假阳性率\n",
    "# # np.unique() 该函数是去除数组中的重复数字，并进行排序之后输出。\n",
    "# # print(np.concatenate([fpr[i] for i in range(n_classes)]).shape) (16324,)\n",
    "# all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))  # (7901,)\n",
    "# # 然后插值所有的ROC曲线在这一点\n",
    "# # np.zeros_like() 这个函数的意思就是生成一个和你所给数组a相同shape的全0数组。\n",
    "# mean_tpr = np.zeros_like(all_fpr)\n",
    "# for i in range(n_classes):\n",
    "#     mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "    \n",
    "# # 最后求平均值并计算AUC\n",
    "# mean_tpr /= n_classes\n",
    "# fpr[\"macro\"] = all_fpr\n",
    "# tpr[\"macro\"] = mean_tpr\n",
    "# roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# # Plot all ROC curves\n",
    "# plt.figure(1)\n",
    "# plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='deeppink', linestyle=':', linewidth=4,\n",
    "#          label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]))\n",
    "\n",
    "# plt.plot(fpr[\"macro\"], tpr[\"macro\"],color='navy', linestyle=':', linewidth=4,\n",
    "#          label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"macro\"]))\n",
    "\n",
    "# colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "# for i, color in zip(range(n_classes), colors):\n",
    "#     plt.plot(fpr[i], tpr[i], color=color, linewidth=2,\n",
    "#              label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Some extension of Receiver Operating Characteristic to multi-class')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # Zoom in view of the upper left corner.\n",
    "# plt.figure(2)\n",
    "# plt.xlim(0, 0.2)\n",
    "# plt.ylim(0.8, 1)\n",
    "# plt.plot(fpr[\"micro\"], tpr[\"micro\"],color='deeppink', linestyle=':', linewidth=4,\n",
    "#          label='micro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"micro\"]))\n",
    "\n",
    "# plt.plot(fpr[\"macro\"], tpr[\"macro\"],color='navy', linestyle=':', linewidth=4,\n",
    "#          label='macro-average ROC curve (area = {0:0.2f})'.format(roc_auc[\"macro\"]))\n",
    "\n",
    "# colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "# for i, color in zip(range(n_classes), colors):\n",
    "#     plt.plot(fpr[i], tpr[i], color=color, linewidth=2,\n",
    "#              label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "# plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC curve (zoomed in at top left)')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAZqqkEue6NU"
   },
   "source": [
    "#### 混淆矩阵 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaS4QgDge6NW"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# def plot_confusion_matrix(title, y_true, y_pred, labels):\n",
    "#     cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "#     # np.newaxis的作用就是在这一位置增加一个一维，这一位置指的是np.newaxis所在的位置，比较抽象，需要配合例子理解。\n",
    "#     # x1 = np.array([1, 2, 3, 4, 5])\n",
    "#     # the shape of x1 is (5,)\n",
    "#     # x1_new = x1[:, np.newaxis]\n",
    "# # now, the shape of x1_new is (5, 1)\n",
    "\n",
    "\n",
    "#     cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#     # print (cm, '\\n\\n', cm_normalized)\n",
    "#     # [[1 0 0 0 0]                           \n",
    "#     #  [0 1 0 0 0]\n",
    "#     #  [0 0 1 0 0]\n",
    "#     #  [0 0 0 1 0]\n",
    "#     #  [0 0 0 0 1]]\n",
    "\n",
    "#     #  [[1. 0. 0. 0. 0.]\n",
    "#     #  [0. 1. 0. 0. 0.]\n",
    "#     #  [0. 0. 1. 0. 0.]\n",
    "#     #  [0. 0. 0. 1. 0.]\n",
    "#     #  [0. 0. 0. 0. 1.]]\n",
    "#     tick_marks = np.array(range(len(labels))) + 0.5\n",
    "#     #  [0.5 1.5 2.5 3.5 4.5 5.5]\n",
    "#     np.set_printoptions(precision=2)\n",
    "    \n",
    "#     plt.figure(figsize=(10, 8), dpi=120)\n",
    "#     ind_array = np.arange(len(labels))\n",
    "#     x, y = np.meshgrid(ind_array, ind_array)\n",
    "#     # print(ind_ａrray, '\\n\\n', x, '\\n\\n', y)\n",
    "#     # [0 1 2 3 4 5] \n",
    "\n",
    "#     #  [[0 1 2 3 4 5]\n",
    "#     #  [0 1 2 3 4 5]\n",
    "#     #  [0 1 2 3 4 5]\n",
    "#     #  [0 1 2 3 4 5]\n",
    "#     #  [0 1 2 3 4 5]\n",
    "#     #  [0 1 2 3 4 5]] \n",
    "\n",
    "#     #  [[0 0 0 0 0 0]\n",
    "#     #  [1 1 1 1 1 1]\n",
    "#     #  [2 2 2 2 2 2]\n",
    "#     #  [3 3 3 3 3 3]\n",
    "#     #  [4 4 4 4 4 4]\n",
    "#     #  [5 5 5 5 5 5]]\n",
    "#     intFlag = 0 # 标记在图片中对文字是整数型还是浮点型\n",
    "#     for x_val, y_val in zip(x.flatten(), y.flatten()):\n",
    "#         # plt.text()函数用于设置文字说明。\n",
    "\n",
    "#         if (intFlag):\n",
    "#             c = cm[y_val][x_val]\n",
    "#             plt.text(x_val, y_val, \"%d\" % (c,), color='red', fontsize=8, va='center', ha='center')\n",
    "\n",
    "#         else:\n",
    "#             c = cm_normalized[y_val][x_val]\n",
    "#             if (c > 0.01):\n",
    "#                 plt.text(x_val, y_val, \"%0.2f\" % (c,), color='red', fontsize=7, va='center', ha='center')\n",
    "#             else:\n",
    "#                 plt.text(x_val, y_val, \"%d\" % (0,), color='red', fontsize=7, va='center', ha='center')\n",
    "#     cmap = plt.cm.binary\n",
    "#     if(intFlag):\n",
    "#         plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     else:\n",
    "#         plt.imshow(cm_normalized, interpolation='nearest', cmap=cmap)\n",
    "#     plt.gca().set_xticks(tick_marks, minor=True)\n",
    "#     plt.gca().set_yticks(tick_marks, minor=True)\n",
    "#     plt.gca().xaxis.set_ticks_position('none')\n",
    "#     plt.gca().yaxis.set_ticks_position('none')\n",
    "#     plt.grid(True, which='minor', linestyle='-')\n",
    "#     plt.gcf().subplots_adjust(bottom=0.15)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     xlocations = np.array(range(len(labels)))\n",
    "#     plt.xticks(xlocations, labels, rotation=90)\n",
    "#     plt.yticks(xlocations, labels)\n",
    "#     plt.ylabel('Index of True Classes')\n",
    "#     plt.xlabel('Index of Predict Classes')\n",
    "#     plt.savefig('confusion_matrix.jpg', dpi=300)\n",
    "#     plt.show()\n",
    "# title='Confusion Matrix'\n",
    "# labels = ['A', 'B', 'C', 'F', 'G']\n",
    "# y_true = \n",
    "# y_pred = [1, 2, 3, 4, 5]# np.loadtxt(r'/home/dingtom/b.txt')\n",
    "# plot＿confusion_matrix(title, y_true,y_pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5T2uH7cfe6Nl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNkZObQ1e6Nr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LtKVNGRke6N-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SjJe1fnpe6OV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "keras_cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tomding",
   "language": "python",
   "name": "tomding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
